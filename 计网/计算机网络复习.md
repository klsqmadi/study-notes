## 计算机网络复习

### 网络层

网络层向上提供了一种尽力而为服务，将各种确保交付等功能交给了上层实现。

网络层的主要功能：**转发**、**路由选择**，它需要做到尽量快的将数据从入口转发到出口。

#### 最长前缀匹配规则

路由器中有一个转发表，它的表项分别包含了**子网前缀**以及去向该子网的**链路接口**

| 前缀匹配                   | 链路接口 |
| -------------------------- | -------- |
| 11001000 00010111 00010    | 0        |
| 11001000 00010111 00011000 | 1        |
| 11001000 00010111 00011    | 2        |
| 其他                       | 3        |

当路由器收到需要转发的报文段，它会根据其目的地址进行最长前缀匹配，找到能与该目的地址匹配前缀最长的表项，然后把数据转发到对应的链路接口。

也就是说，网络层的寻址是一种**层次化**的，路由器只负责将报文段发送给这个目的地址所在的子网，一层一层进行传递。

#### IPv4

##### 编址

IPv4 协议的地址共 4 个字节，以十进制表示，主机、路由器的每个接口都会有自己的 IP 地址。

编址有两种方式：**CIDR 编址** ，子网划分和 **分类编址**。

- 分类编址

  将组织分为了 A、B、C 三类，它们的前半部分分别是 8、16、24 位，也就是 A 类有 24 位的可用 IP，B 类有 16 位可用 IP，C 类则有 8 位。这样编址的最大问题就是有对资源的浪费问题（有时候 B 类太大了，C 类又太小了）

- ##### 子网划分

  **通过在主机号字段中拿一部分作为子网号**，把两级 IP 地址划分为**三级 IP 地址。**

  ```
  IP 地址 ::= {< 网络号 >, < 子网号 >, < 主机号 >}
  ```

  要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。

  注意，外部网络看不到子网的存在。

- CIDR

  无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用**网络前缀和主机号**来对 IP 地址进行编码，网络前缀的长度可以根据需要变化。

  ` IP 地址 ::= {< 网络前缀号 >, < 主机号 >} `

  CIDR 的写法上采用在 IP 地址后面加上网络前缀长度的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。

  CIDR 的地址掩码可以继续称为**子网掩码**，子网掩码首 1（连续1） 长度为网络前缀的长度。

##### 分片

由于某些链路能承载的数据大小有限，因此 IPv4 支持了对数据报进行分片，再将其交给链路层。当目的主机接受完所有的片后，就会将这些片重新组合拼接位原始数据报。

IPv4 通过如下对分片进行了支持。

- 标识字段表示同一数据报
- 标志字段（`保留位`，`不分片位`：若要分片发送icmp报文，`更多片位`)
- 片偏移字段

由于分片比较消耗性能，并且网络层的主要目的就是尽快的对数据报进行了转发，因此 **IPv6 取消了对分片的支持，将分片的功能交给了上层去实现，仅仅通过 ICMP 报文段通知上层**。

##### 寿命

由于数据报存在在网络中循环的风险，因此 IPv4 包含了一个**寿命**字段，每当一个路由器这个数据报，寿命就会减 1，当寿命为 0 时，数据报就会被丢弃，从而解决了数据报的无限循环问题。

#### DHCP

子网对 IP 地址的分配是需要主机手动进行配置的，这非常麻烦，因此可以通过 DHCP 协议解决，DHCP 协议可以实现 IP 地址的动态分配。

DHCP 是一个**应用层协议**，它**基于 UDP**，主机可以通过 DHCP 自动获取一个 IP 地址，它使用了 68 端口进行发送，67 端口进行接收。

DHCP 的步骤如下：

1. 新到达的机器发送一个**DHCP 发现报文段**，它基于 UDP 采用了广播目的地址 255.255.255.255，并且填入本机源地址 0.0.0.0，并且其端口号为 68。链路层会将这个帧广播到所有这个机器所在子网中的所有机器中。
2. DHCP 服务器收到了 DHCP 发现报文段，会通过 DHCP 提供报文段向客户响应。由于不知道客户的目的地址，因此也是通过广播的形式，并且源地址中填入了自己的地址。
3. 子网中不止存在一台 DHCP 服务器，因此该机器会收到多个 DHCP 发现报文段。它选择其中一个后，会向该服务器发送一个 DHCP 请求报文段。
4. DHCP 服务器收到请求报文段后，会通过 DHCP ACK 报文段向该机器进行回应，证实所需要的参数，此时该机器就成功地分配到了一个自己的 IP 地址。

存在一种 DHCP 饥饿攻击，它类似 SYN 洪泛攻击，攻击方通过仿冒 MAC 不断向 DHCP 服务器请求 IP 地址，使得子网的 IP 资源耗尽，无法正常进行 IP 分配。

#### ICMP

ICMP 协议用于在主机、路由器之间沟通网络层信息，它典型的用途就是差错报告（如网络不可达等），它**基于 IP 协议实现**，但仍然属于网络层的协议。

ICMP 报文段由**类型**、**编码**两部分共同构成。主机收到了指明上层为 ICMP 的 IP 报文段后，就会拆解 ICMP 报文段的部分，根据类型和编码确定其传递的信息并处理。

##### ping 程序

ping 程序基于 ICMP 实现，它通过**回显请求报文段**发送到指定主机，对应主机收到后就会返回一个**回显回答报文段**从而实现 ping 的功能。

##### traceroute 程序

traceroute 通过 ICMP 配合 IPv4 的寿命字段实现，它通过发送从 1 开始不断递增的报文段，当第 n 个报文段到达第 n 个路由器时，路由器就会通过 ICMP 返回一个寿命过期的报文段，从该报文段中就可以解析出第 n 个路由器的 IP 地址。并且通过对每个报文段进行计时，就可以获取到往返时延。

#### NAT

NAT 的核心思路就是一群设备共用同一个 IP 地址，**通过端口号来对不同设备进行区分**。它依赖于具有 NAT 功能的路由器，NAT 路由器负责将 Internet 中的信息根据到达的端口号进行转发，它自己的 IP 由 DHCP 获得，同时这个路由器内部也运行着一个 DHCP 服务器用来为路由器控制下的设备提供内部 IP。

它内部维护了一张 **NAT 转换表**，包含了 WAN 端（外部）和 LAN 端（内部）的端口号及地址：

| WAN端            | LAN端         |
| ---------------- | ------------- |
| 138.76.29.7:5001 | 10.0.0.1:3345 |
| ......           | ......        |

这张 NAT 转换表表示了从外网中的端口号到 NAT 网络中设备的映射，NAT 路由器负责在网络请求的过程中对报文段的 IP 地址和端口号进行替换，并且将报文段进行转发，从而让 NAT 网络中的设备就像是真的在 Internet 中一样。

NAT 可以有效的解决 IPv4 地址缺乏问题，它通过端口号对可用的地址进行扩充，从而使得即使 IPv4 地址耗尽，仍然能有新的设备加入网络。

但是它存在着如下的问题：

- **NAT 的设计不是很合理**。端口号本应当是用于为进程编址的，但 NAT 却将端口号也作为了主机编址的一部分。
- **NAT 会影响网络速率**。由于数据的传输过程中还需要 NAT 设备对数据包进行修改，这会降低数据传输的效率。
- **有的协议无法通过 NAT 进行传输**。

#### IPv6

IPv6 协议对编址的位数进行了扩充，有 128 位，同时还对 IPv4 存在的不足进行了优化。

- **地址容量更大**：IPv6 的地址有 128 位，并且引入了**任播地址**，这种地址可以实现数据报交给一组机器中的任意一个。
- **舍弃了分片/重组功能**：IPv6 不允许在中间的路由器进行分片，如果遇到了链路无法承载的数据报，则通过 ICMP 通知发送方的端系统，由端系统进行数据的分片并重新发送数据。也就是说**分片/组装的工作交给了端系统**。
- **首部固定长度**：IPv6 舍弃了许多 IPv4 中的字段，它的首部具有 40 字节的**固定长度**，可以使得对 IP 数据报的处理更快。
- **流标签与优先级**：IPv6 引入了**流**的概念，例如音频、视频的传输就可以当作一个流，并且引入了流量类型字段用于区分不同类型的流，可以通过流量类型而对某些流量赋予更高的优先级。

##### IPv4 到 IPv6 的迁移

全球统一迁移显然是不现实的，目前主要的 IPv4 向 IPv6 迁移的解决方案有如下几种：

- **双栈**：使用同时具有 IPv4、IPv6 的完整功能的节点，当 IPv4 结点互操作时，使用 IPv4，当 IPv6 结点互操作时，使用 IPv6。要确定另一个节点的类型可以通过 DNS 来解决，根据请求的节点类型返回对应的 IP 地址。这样双方只要有一个是只支持 IPv4 的，则需要使用 IPv4 协议来进行。
- **建隧道**：在只支持 IPv4 的节点上通过 IPv4 数据报来传递 IPv6 数据报，将 IPv6 数据报视为 IPv4 数据报的一部分。

#### 路由选择算法

路由选择算法主要有距离向量（DV）和链路状态（LS）两种。

##### 距离向量（DV）算法

是一种迭代、分布式的算法，**每个节点只从其邻居 **处接收信息，并进行计算。

每个节点会不断向它的邻居发送自己的距离向量副本。

每当节点从邻居处收到新的距离向量或者它到邻居的费用发生改变时，则会通过下面的方程更新自己的距离向量：

`d(x, y) = minv(c(x, v) + d(v, y))	// 对N中每个结点`

之后如果该距离向量发生了改变，则会将这个距离向量通知给所有的邻居。

在 DV 算法中，好消息（费用降低）是传递的非常快的，只需要很少的几次交流整个网络就会回归静止。

而对于坏消息（费用提高）来说，它传递的是非常慢的。因为费用的提高会造成一种**选择环路**问题，两个节点由于不清楚整体局势，都认为通过对方发送数据是一条最低费用路径，因此导致消息在两个节点中来回传递。

##### 链路状态（LS）算法

链路状态算法中，**网络拓扑以及所有链路的费用对每个节点都是已知的**，这往往通过一种**链路状态广播**实现。

它内部所使用的路由选择算法为 **Dijkstra 算法**，其最坏的时间复杂度为 O(n^2)。

#### 路由选择协议

##### RIP（DV，AS内）

RIP 是一种基于距离向量（DV）算法实现的协议，是一种自治系统内部的路由选择协议，它用跳数来作为费用测度，**每条链路的费用都被认为是 1**。

它使用 **RIP 响应报文段**来实现邻居之间的信息交换，每台路由器都维护了一张 RIP 表，它包括了这个路由器的距离向量和这个路由器的转发表：

| 目的子网 | 下一台路由器 | 到目的地的跳数 |
| -------- | ------------ | -------------- |
| w        | A            | 2              |
| y        | B            | 2              |
| z        | B            | 7              |
| x        | -            | 1              |
| ...      | ...          | ...            |

RIP 路由器大概每 30 秒会进行一次交互通告，如果一台路由器超过 180 秒没有从邻居处收到报文段，则该邻居不再认为是可达的，此时该路由器会修改本地路由选择表，然后向相邻路由器发送通告来传播该消息。 

RIP 协议还支持通过发送 **RIP 请求报文段**来获取它的邻居到达目的地的费用。

RIP 协议是**基于 UDP 实现**的，因此它是一个应用层的报文段，往往由操作系统实现。

##### OSPF（LS，AS内）

OSPF 协议基于链路状态（LS）算法实现，它的**各条链路的费用由网络管理员配置**。

OSPF 路由器会**向该 AS 内所有其他路由器广播路由选择信息**，而不只是向邻居传递。当链路状态改变时，路由器就会将链路状态信息进行广播。即使没有变化，它也会定时进行一次链路状态广播。

**OSPF 协议基于 IP 协议实现**，它还支持通过向邻居发送 HELLO 报文段实现对链路的检查，以及对网络范围内的链路状态的获取。

OSPF 还可以将自治系统分为多个区域，每个区域运行自己的 OSPF 算法，区域之间由**区域边界路由器**负责对外进行路由选择。同时会在这些区域内存在一个主干区域，这个区域负责为不同区域之间的流量提供路由选择。

AS 内的区域间路由选择分组首先路由到一个区域边界路由器，再通过主干路由到位于目的区域的区域边界路由器，最后路由到目的地。

##### BGP（AS外）

BGP 协议用于支持 AS 外的路由选择，其中，跨越两个 AS 的会话叫做外部 BGP（eBGP），而同一个 AS 内的会话称为（iBGP）。

BGP 协议可以让 AS 知道通过它相邻的 AS 能到达哪些子网。

网关之间的 eBGP 会话，会发送当前 AS 可达到前缀列表，当网关路由器收到前缀列表后，会通过 iBGP 会话将这些前缀向 AS 内的各个路由器进行转发，每台路由器收到前缀列表后，都会将其加入转发表。

### 链路层

#### MAC 地址

MAC 地址（LAN 地址）是一种与地域无关，每个网络适配器生产时就决定的地址，它有 6个字节，一般用十六进制表示。

适配器发送一个帧时，它会将目的 MAC 地址填入发送的帧，并将其传输到**局域网**，局域网内的所有适配器都会收到这个帧，但如果这个帧的目的 MAC 地址不是自己，并且不是广播帧的地址（FF-FF-FF-FF-FF-FF），则会将这个帧丢弃，否则它会向上层协议传递。

#### ARP

要发送一个 IP 数据报，我们首先是需要知道其 IP 地址，但同时在链路层也需要知道它的 MAC 地址。ARP 协议就是一种用于实现 IP 地址到 MAC 地址转换的协议，它是一个跨越了网络层和链路层的协议，它内部维护了一个 **ARP 表**，每个表项都包含了 **IP 地址**、**MAC 地址**以及**过期时间**三项，表达了一个 IP 地址到 MAC 地址的映射，并且当到达过期时间时，对应表项会被移除。

| IP 地址         | MAC 地址          | TTL      |
| --------------- | ----------------- | -------- |
| 222.222.222.221 | 88-B2-2F-54-1A-0F | 13:45:00 |
| 222.222.222.223 | 5C-66-AB-90-75-B1 | 13:52:00 |

在进行对 IP 到 MAC 的转换时：

- 首先会尝试在 ARP 找中找到对应的映射，并解析为对应的 MAC 地址。
- 若 ARP 表中不存在对应的映射，发送方会构造一个 **ARP 查询帧**，在目的 MAC 地址中填入广播地址（FF-FF-FF-FF-FF-FF）并将其传输到局域网
- 局域网中的其他适配器都能收到这个查询报文段，如果他们发现目的 IP 地址与自己的不符，则会丢弃该帧。
- 如果目的 IP 地址相匹配，则会给查询主机发送一个附带了映射信息的 ARP 响应帧（由于已经有发送方的源 MAC 地址，不需要再通过广播的方式）
- 查询主机收到映射后，会更新其 ARP 表。



![image-20191218113227536](https://gitee.com/a2547555298/imags/raw/master/006tNbRwgy1ga0pwtx2lkj30xt07vt9g.jpg)

### 有了 MAC 地址为什么还需要 IP 地址？

核心原因在于：**网络层寻址是分层次的，而链路层寻址则是扁平的**。

首先，我们都知道，MAC 地址是网络适配器生产的时候就已经决定的，与地域无关。也就是说我们无法通过 MAC 地址去确定它所处的局域网，从而让路由器确定该如何转发。如果我们想要让一个报文段能够通过 MAC 地址实现对整个全球互联网中的其他机器进行数据传输，那么我们必须要知道每个 MAC 地址所对应的局域网，也就是说每台路由器中都需要记录下世界上所有的 MAC 地址所对应的位置，这显然不现实。

而 IP 地址则是由最上级开始一级一级划分子网从而派发的，它具有地域性，同一个子网下的设备的 IP 地址前缀是相同的，通过这个前缀的匹配，我们可以确定 IP 地址所对应的子网，而路由器也只需要记录对应子网对应的接口即可。

我们可以将 MAC 地址当作我们的身份证，而 IP 地址则可以当作我们的一个地址。显然快递员（路由器）是无法通过我们的身份证号来找到我们具体的位置并将快递（报文段）交给我们的，除非他们知道世界上每一个身份证号对应的地址。而他们则可以通过地址由发送地一级一级转发，根据地址来确定我们的具体位置并交给我们。

#### 链路层交换机

链路层交换机的作用是对链路层的帧进行接收并转发到出链路。

交换机内维护了一个**交换机表**，表项中包含了 MAC 地址以及该 MAC 地址对应的接口，同时还包含了到达时间。

| 地址              | 接口 | 时间 |
| ----------------- | ---- | ---- |
| 62-FE-F7-11-89-A3 | 1    | 9:32 |
| 7C-BA-B2-B4-91-10 | 3    | 9:36 |
| ...               | ...  | ...  |

它的交换机表默认状态下为空，每当交换机收到一个新的帧的时候，它都会在交换机表中记录下这个帧的**源地址**、**到达的接口**以及**当前的时间**，因为能从这个接口到达就意味着可以从这个接口将数据转发到对应的源网络适配器。

#### 交换机与路由器的对比

交换机与路由器非常相似，它们都是存储转发分组交换机，但路由器是基于网络层地址（IP）进行转发，而交换机是基于链路层地址（MAC）进行转发。

交换机具有如下的特点

- 优点
  - 即插即用
  - 较高的分组过滤和转发速率
- 缺点
  - 主机和路由器中需要有大量的 ARP 表
  - 对广播风暴问题不提供任何保护措施

而路由器则具有以下的特点：

- 优点
  - 允许以丰富的拓扑结构构建因特网
  - 有着更健壮的流量隔离方式和对广播风暴的控制。
- 缺点
  - 并非即插即用，需要人为对路由器和主机进行配置（DHCP 可以解决）
  - 分组的处理速率相对更慢

### HTTP

Http是基于TCP/IP协议的应用层超文本传输协议，主要规定了客户端和服务器的通信格式，默认使用80端口。

#### 特点

1. **灵活可拓展**，因为语法上只规定了基本格式，用空格分隔单词，换行分隔字段。以及传输形式多样，可以是文本，图片，视频等任意数据
2. **请求应答模型**，客户端发出请求，服务端进行响应
3. **无状态**，每次请求都与上一次请求状态无关
4. **可靠传输**，HTTP是基于TCP的，因为TCP是可靠传输协议

缺点

1. 不安全

#### 各版本差异

##### HTTP 0.9

- 只支持GET命令传输简单文本

##### HTTP 1.0

1. 新增POST，HEAD命令
2. **任何类型的内容都可以发送**
3. 请求和回应格式改变，除了数据部分，必须加上头信息描述一些元数据
4. 短连接，HTTP/1.0每进行一次HTTP通信，都需要经历建立TCP连接、传输HTTP数据和断开TCP连接三个阶段
6. 每台计算机只能绑定一个域名，所以请求消息中的 URL 并没有传递主机名（hostname）

#####  HTTP 1.1

HTTP 1.1是目前最为主流的HTTP 协议版本

1. **持久化连接**，即TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive
2. 新增PUT,DELETE,OPTION,PATCH方法
3. 引入**管道机制**，将多个HTTP请求整批提交，不用等待服务端的响应
4. 支持**断点续传**，通过请求头中的Range实现。
5. 使用了**虚拟网络**，在一台物理服务器上可以存在多个虚拟主机（域名），并且它们共享一个IP地址。

缺点

1. **队头阻塞**仍然存在。管道机制虽然可以整批发送请求，不过服务器依然需要根据请求顺序来回复浏览器的请求。可以通过并发连接缓解，因为HTTP协议和浏览器对连接数量的限制，还可以通过域名分片使多个域名指向同一个服务器提高连接数。但并不能从根本上解决问题，耗费资源。

   > **队头阻塞**：对于每一个HTTP请求而言，这些任务是会被放入一个任务队列中串行执行的，一旦队首任务请求太慢时，就会阻塞后面的请求处理，这就是`HTTP队头阻塞`问题。

2. Header不会压缩，网络流量浪费

3. **单向请求：** 只能单向请求，客户端请求什么，服务器返回什么；

##### HTTP2.0

1. 二进制分帧
2. 头部压缩
3. 多路复用
4. 服务器推送
5. 请求优先级

**二进制分帧**

HTTP采用二进制传输数据，而非HTTP1.x的文本格式，二进制解析起来更高效。

**在HTTP2.0中帧是数据传输的最小单位，以二进制代替明文传输，原本的报文消息被划分为更小的帧。**帧分为两种Headers Frame和Data Frame。

![image](https://gitee.com/a2547555298/imags/raw/master/1658dc4b44118517.png)

**多路复用**

多路复用指一个域名上的请求**共用一个TCP连接**,在TCP连接上每发起一个请求，就会建立一个**数据流**。

请求/响应会分成多个帧在**stream**上传输，每个帧都有对应的stram Id，在收到帧的时候会根据strem id拼**成完整的HTTP报文。**因此可以让多个请求可以**同时进行**，提高数据传输效率。

![img](https://gitee.com/a2547555298/imags/raw/master/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png)

**实现机制**

![img](https://gitee.com/a2547555298/imags/raw/master/50cd9acb229f4e3b85523dcd600d1544~tplv-k3u1fbpfcp-zoom-1.image)

添加了一个**二进制分帧层**，将消息传到二进制分帧层进行分帧和拼接。通过引用二进制分帧层，HTTP层语义依然是一样的，改变的只是传输方式。

**头部压缩**

HTTP1.x的多个请求和响应中header中的字段有很多重复的，例如`method:get`、`status:200`等等。这些字段消耗不必要的资源，因此HTTP2.0使用Hpack进行压缩

**Hpack**

维护索引表，传输过程使用索引表示消息，让Header字段得到极大的精简和复用

- 消息发送和接受端共同维护一份静态表和一份动态表
- 每次发送时，发送方根据**字典内容**和**哈夫曼编码**压缩消息头部
- 接受方根据字典解码，并且根据指令判断是否要更新动态表

**静态表**：只包含常见的**头部名称**以及**头部名称和值的组合**

**动态表**：动态表最初是一个空表，当每次解压头部的时候，根据指令动态维护

![img](https://gitee.com/a2547555298/imags/raw/master/Hpack.png)

[详解http-2头部压缩算法](https://segmentfault.com/a/1190000017011816)

**服务器推送**

浏览器发送一个请求，服务器主动向浏览器推送与这个请求相关的资源，这样浏览器就不用发起后续请求。

>  HTTP/2.0 中由于支持了服务器推送，因此当用户请求了 HTML 时，服务器可以将这个 HTML 文件及可能会用到的静态资源一并推送给客户端。

**请求优先级**

 可以设置stram的优先级，**让服务端先处理重要资源，优化用户体验。**

**缺点**：`HTTP 2`中，多个请求在一个 TCP 中的，出现了丢包时，`HTTP 2`的表现反倒不如`HTTP 1.1`了。因为 TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要**等待**重新传输确认，`HTTP 2`出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该 TCP 连接中的所有请求。而对于 `HTTP 1.1` 来说，可以开启多个 TCP 连接，出现这种情况反倒只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据

[HTTP1、HTTP2、HTTP3](https://juejin.cn/post/6855470356657307662#heading-2)

[HTTP2 详解](https://juejin.cn/post/6844903667569541133#refetch)

[HTTP2 详解](https://juejin.cn/post/6844903667569541133#heading-8)

#### 报文结构

分为请求报文及响应报文，分别如下：

**请求报文：**

![](https://gitee.com/a2547555298/imags/raw/master/006tNbRwgy1ga6fwmszi0j30it08kmxj.jpg)

**响应报文：**

![](https://gitee.com/a2547555298/imags/raw/master/%E5%93%8D%E5%BA%94%E6%8A%A5%E6%96%87.png)

**URL**统一资源定位符的简称，Uniform Resource Locator，常常被称为网址，是因特网上标准的资源地址。

**组成**

通用的格式：scheme://host[:port]/path/…/?query#anchor

| 名称         | 功能                                                         |
| ------------ | ------------------------------------------------------------ |
| scheme       | 访问服务器以获取资源时要使用哪种协议，比如：http，https 和 FTP 等 |
| host         | HTTP 服务器的 IP 地址或者域名                                |
| port         | HTTP 服务器的默认端口是 80，HTTPS默认端口是443，这种情况下端口号可以省略，如果使用了别的端口，必须指明。不同的端口，你可以认为是不同的应用程序。 |
| path         | 访问资源的路径                                               |
| query-string | 发给 http 服务器的数据                                       |
| anchor       | 锚点                                                         |

https://www.baidu.com/s?tn=baidu&bar=&wd=TianTian

**URL 编码**

- URL 只能使用 [ASCII 字符集](https://www.w3school.com.cn/tags/html_ref_ascii.asp)来通过因特网进行发送。
- 由于 URL 常常会包含 ASCII 集合之外的字符，URL 必须转换为有效的 ASCII 格式。
- URL 编码使用 "%" 其后跟随两位的十六进制数来替换非 ASCII 字符。
- URL 不能包含空格。URL 编码通常使用 + 来替换空格。

举个例子👇

```
天天`转换为有效的ASCII格式就是`%CC%EC%CC%EC
```

#### header

- **通用首部字段（General Header Fields）**

  请求报文和响应报文两方都会使用的首部。

  | 首部字段名        | 作用                       |                             说明                             |
  | :---------------- | -------------------------- | :----------------------------------------------------------: |
  | Cache-Control     | 控制缓存的行为             |                                                              |
  | Connection        | 连接的管理                 | keep-alive表示要求服务器不要关闭TCP连接，close表示明确要求关闭连接，1.1开始默认值是keep-alive |
  | Transfer-Encoding | 指定报文主体的传输编码方式 | chunked表示采用分块传输编码，有该字段则无需使用Content-Length字段。 |
  | Upgrade           | 升级为其他协议             |                                                              |

- **请求首部报文（Request Headers Fields）**

  从客户端向服务端发送请求报文时使用的首部。补充了请求的附加内容，客户端信息，响应内容相关优先级等信息。

  ### 请求首部字段

  | 首部字段名        | 说明                         |
  | ----------------- | ---------------------------- |
  | Accept            | 客户端可以识别的内容类型列表 |
  | Accept-Encoding   | 优先的内容编码               |
  | Expect            | 期待服务器的特定行为         |
  | Host              | 请求资源所在服务器           |
  | If-Modified-Since | 比较资源的更新时间           |
  | If-None-Match     | 比较实体标记                 |
  | Range             | 实体的字节范围请求           |
  | User-Agent        | HTTP客户端程序的信息         |
  
- **响应首部字段（Response Header Fields）**

  从服务器端向客户端返回响应报文时使用的首部。补充了响应的附加内容，也会要求客户端附加额外的内容信息。

- **实体首部字段（Entity Header Fields）**

  针对请求报文和响应报文的实体部分使用的首部。补充了资源内容更新时间等与实体相关的信息。

  请求头和响应头的部分字段：



- Host：指定服务器域名，可用来区分访问一个服务器上的不同服务
- User-Agent：用户代理，是服务器能识别客户端的操作系统（Android、IOS、WEB）及相关的信息。作用是帮助服务器区分客户端，并且针对不同客户端让用户看到不同数据，做不同操作。
- Content-Type：服务器告诉客户端数据的格式，常见的值有text/plain，image/jpeg，image/png，video/mp4，application/json，application/zip。这些数据类型总称为MIME TYPE。
- Content-Encoding：服务器数据压缩方式
- Content-Length：声明数据的长度，请求和回应头部都可以使用该字段。

**数据传输**

分为**「定长数据」** 与 **「不定长数据」**

对于定长的数据包而言，发送端在发送数据的过程中，需要设置`Content-Length`,来指明发送数据的长度。

- Content-Length如果存在并且有效的话，则必须和消息内容的传输长度完全一致，也就是说，如果过短就会截断，过长的话，就会导致超时。
- 如果采用短链接的话，直接可以通过服务器关闭连接来确定消息的传输长度。
- 在HTTP/1.1版本中，如果是Keep-alive的话，chunked优先级高于`Content-Length`,若是非Keep-alive，Content-Length可有可无。

**不定长数据**

现在采用最多的就是HTTP/1.1版本，来完成传输数据，在保存Keep-alive状态下，当数据是不定长的时候，我们需要设置新的头部字段👇

```
Transfer-Encoding: chunked
```

通过chunked机制，可以完成对不定长数据的处理，当然了，你需要知道的是

- 如果头部信息中有`Transfer-Encoding`,优先采用Transfer-Encoding里面的方法来找到对应的长度。
- 如果设置了Transfer-Encoding，那么Content-Length将被忽视。
- 使用长连接的话，会持续的推送动态内容。



#### 常见状态码

| 状态码 | 类别             | 描述                       |
| ------ | ---------------- | -------------------------- |
| 1XX    | 信息性状态码     | 接收的请求正在处理         |
| 2XX    | 成功状态码       | 请求正常处理完毕           |
| 3XX    | 重定向状态码     | 需要进行附加操作以完成请求 |
| 4XX    | 客户端错误状态码 | 服务器无法处理请求         |
| 5XX    | 服务端错误状态码 | 服务器处理请求出错         |

**2xx成功：**

- `204 No content` 表示请求成功，但没有资源可返回
- `206 Partial Content` 该状态码表示客户端进行了范围请求，服务器成功执行，响应中包含了`Content-Length`指定的实体内容。

**3xx重定向：**

- `301 Moved Permantenly`：表示永久性重定向，也就是这个资源被分配了新的 URI，以后需要使用这个资源所指的 URI，此时需要通过 `Location` 中拿到的 URI 重新请求。
- `302 Found`：表示临时性重定向，希望用户在本次中使用 `Location` 指定的 URI 使用。
- `304 Not Modified`：表示资源没有被修改（与重定向无关）。~~需要配合 `If-Match`，`If-ModifiedSince`，`If-None-Match`，`If-Range`，`If-Unmodified-Since` 等 Header 使用。~~

***4XX 客户端错误*：**

- `400 Bad Request`：说明请求报文存在语法错误，需修改请求的内容后再次发送。
- `401 Unauthorized`：表示发送的请求需要有通过 HTTP 认证的信息，响应的 Header 中会包含一个 `WWW-Authenticate` Header 用来质询用户信息。（例如浏览器有时候弹出的认证对话框）
- `403 Forbidden`：表示服务器拒绝了客户端对该资源的访问
- `404 Not Found` ：我们最常见的一个 4XX 了，表示服务器上找不到对应的资源。

***5XX 服务器错误*：**

- `500 Internal Server Error`：表示服务端执行请求时出现了错误。
- `503 Service Unavailable`：表示服务器暂时处于超负载或维护状态，可能会包含 `RetryAfter` 这个 Header 来告诉客户端何时能够进行访问。

#### 请求方法

- GET：请求指定的页面信息，并返回实体主体。
- HEAD：请求获取由Request-URI所标识的资源的响应消息报头
- POST：向指定资源提交数据**进行处理请求**（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。
- PUT： 请求服务器存储一个资源，并用Request-URI作为其标识（修改数据）
- DELETE： 请求服务器删除对应所标识的资源
- TRACE： 请求服务器回送收到的请求信息，主要用于测试或诊断
- OPTIONS： 列出可对资源实行的请求方法，用来跨域请求。可以查看服务器的性能。
- CONNECT：1.1 协议中预留给能够将连接改为管道方式的代理服务器
- PATCH：对 PUT 方法的补充，用来对已知资源进行局部更新 。

**从输入网址到获得页面的过程？**

- 浏览器查询 DNS，获取域名对应的IP地址； 
-  浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手； 
- TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求； 
- 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器； 
-  浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源； 
- 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。

#### 常见问题

##### get和post



![w3school](https://gitee.com/a2547555298/imags/raw/master/get%E5%92%8Cpost%E5%8C%BA%E5%88%AB.png)

- **通常**GET用来获取数据，post用来提交数据
- GET参数长度在浏览器和服务器有限制，通常不超过2k字节，post无限制
- 对参数数据类型GET只接受ASCII字符，而POST没有限制。
- GET请求参数会保留在浏览器历史记录中。
- GET是幂等的，幂等就是指同一个请求执行多次和仅执行一次的效果完全相等。
- GET参数通过URL传递，POST放在Request body中。
- GET的回退是无害的，post会被重新提交

**get和post没有本质的区别**，只是**报文格式**的不同。get和post是http协议中的两种方法，而http协议是基于TCP/IP的应用层协议，无论是get还是post都是在用一个传输层协议上传输，没有区别。

报文格式上：

- 不带参数时，仅第一行方法名不同，一个是GET，一个是POST。
- 带参数时，在约定中，GET方法的参数应该放在url中，POST方法的参数应该放在body中。

##### 代理

**正向代理**

常说的代理就是指正向代理，正向代理的过程，它隐藏了真实的请求客户端，服务端不知道真实的客户端是谁，客户端请求的服务都被代理服务器代替来请求。

**反向代理**

这种代理模式下，它隐藏了真实的服务端，当我们向一个网站发起请求的时候，背后可能有成千上万台服务器为我们服务，具体是哪一台，我们不清楚，我们只需要知道反向代理服务器是谁就行，而且反向代理服务器会帮我们把请求转发到真实的服务器那里去，一般而言反向代理服务器一般用来实现负载平衡。

##### 负载平衡

- 一种是使用反向代理的方式，用户的请求都发送到反向代理服务上，然后由反向代理服务器来转发请求到真实的服务器上，以此来实现集群的负载平衡。
- 另一种是 DNS 的方式，DNS 可以用于在冗余的服务器上实现负载平衡。因为现在一般的大型网站使用多台服务器提供服务，因此一个域名可能会对应多个服务器地址。当用户向网站域名请求的时候，DNS 服务器返回这个域名所对应的服务器 IP 地址的集合，但在每个回答中，会循环这些 IP 地址的顺序，用户一般会选择排在前面的地址发送请求。以此将用户的请求均衡的分配到各个不同的服务器上，这样来实现负载均衡。这种方式有一个缺点就是，由于 DNS 服务器中存在缓存，所以有可能一个服务器出现故障后，域名解析仍然返回的是那个 IP 地址，就会造成访问的问题。

#### 中间人攻击

**中间人攻击是一种间接的入侵攻击**，这种攻击模式是将一台计算机**虚拟放置在网络连接中的两台通信计算机之间**，这台计算机就称为“中间人”。

黑客利用这台“中间人”计算机**模拟**相互通信的一端或者两端，使“中间人”能够与原通信计算机建立活动连接并允许其**读取或修改**通信两端所传递的信息。

- DNS欺骗

  攻击者往往可以通过入侵DNS服务器，或是篡改用户本地hosts文件，然后把查询的IP地址设为攻击者的IP地址，从而截获到用户发出的请求。截获请求以后，根据不同目的，攻击者既可以引导用户访问一个**假网站**。也可以把用户请求依旧转发给目标服务器，仅仅实现**监听的目的**。

- **APR欺骗**

  DNS欺骗是在广域网中的拦截用户请求，那么APR欺骗就是在局域网中的拦截用户请求。ARP（Address Resolution Protocol）地址解析协议，是一种将IP地址转化成物理地址的协议。

  攻击者利用了APR协议的漏洞，通过局域网内部的一台主机（IP并不是123），冒充主机B，向主机A发送自己的MAC地址。主机A接到消息以后，无法识别消息是真的来自主机B，还是来自一个冒名顶替者，只能照样把接受到的新MAC地址存入ARP缓存表，取代原先的记录。

  下一次，当主机A想要向主机B发送请求的时候，会先查询自己的ARP缓存表，查出主机B的MAC地址是def（本来应该是abc），结果把请求发给了主机D。从而让攻击者拦截到了请求信息。

  ![img](https://gitee.com/a2547555298/imags/raw/master/19c953ab5c764bd2be3eaf18195f6e97_th.png)

- **代理服务器**

**解决方法：**

- 使用DNSSEC机制
- 使用防火墙和杀毒软件
- 使用HTTPS协议

### HTTPS

HTTPS指 HTTP over SSL，HTTPS并**不是个新的应用层协议**。只是在**HTTP**和**TCP**间加了一层**SSL**，同过SSL进行**加密，身份验证，完整性校验**功能，从而解决HTTP存在的**窃听，伪装，篡改**等问题。

**SSL**(Secure Socket Layer，安全套接字层)

**TLS**(Transport Layer Security，传输层安全)：**其前身是 SSL，SSL升级版**

**窃听问题**：HTTPS的加密采用了对称与非对称加密的**混合加密**，来保证报文的安全性，不会被中间人窃听。通过对称加密对报文进行加密，而对称加密密钥由非对称加密传输，从而保证安全性

**身份验证：**为了保证公钥来自目标服务端，需要通过**CA机构颁发的数字证书进行认证**。数字证书中包含**服务端公钥信息和数字签名**，并用CA机构的私钥进行加密。在操作系统，浏览器中通常自带CA机构的公钥，只需要通过这些公钥进行解密，即可得到服务端公钥

**完整性校验：** 文本用Hash函数生成消息摘要，用CA私钥加密生成数字签名，与原文一起发送给接受者。接收者收到消息后用同样的Hash函数产生摘要，将这两个摘要进行对比，即可验证数据完整性，防止篡改。



![图片](https://gitee.com/a2547555298/imags/raw/master/HTTPS%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86).



**证书**包含以下信息：申请者公钥、申请者的组织信息和个人信息、签发机构 CA的信息、有效时间、证书序列号等信息的明文，同时包含一个签名。其中签名的产生算法：首先，使用散列函数计算公开的明文信息的信息摘要，然后，采用 CA的私钥对信息摘要进行加密，密文即签名;

工作过程

1. 获取公钥：客户端向服务端发起HTTPS请求，端口为443.服务端返回CA机构颁发的数字证书
2. 身份验证：客户端收到数字证书通过CA机构公钥进行解密，从而进行身份验证并获取公钥
3. 生成对称密钥：客户端验证公钥安全后，生成一串随机数作为对称密钥
4. 加密发送对称密钥：客户端用服务端公钥对对称密钥进行加密，发送给服务端
5. 返回数据：服务端用客户端生成的对称密钥对数据进行加密，发送给客户端
6. 解析数据：客户端收到数据后，进行对称解密

**HTTP 与 HTTPS 的区别**

- HTTPS加密传输比HTTP更加安全
- HTTPS需要用到SSL证书，而HTTP不用;
- HTTPS标准端口443，HTTP标准端口80;
- Http协议建立连接的过程比Https协议快。因为Https除了Tcp三次握手，还要经过SSL握手。连接建立之后数据传输速度，二者无明显区别。

**既然 HTTPS 具有安全可靠的特性，为何没有被广泛应用呢？**

1. HTTPS虽然提高了HTTP请求安全性，但同时加密，解密，证书的验证带来了效率影响
2. 很多服务器对安全性要求没这么敏感
3. CA机构提供的数字证书需要花钱购买

#### SSL握手过程

<img src="https://gitee.com/a2547555298/imags/raw/master/ssl%E6%8F%A1%E6%89%8B%E8%BF%87%E7%A8%8B.png" alt="图片" style="zoom:200%;" />

- **Client Hello**	客户端给出协议版本号、一个客户端随机数A（Client random）以及客户端支持的加密方式

- **Server Hello**    服务端确认双方使用的加密方式，并给出数字证书、一个服务器生成的随机数B（Server random）

- **Client Key Exchange** 客户端确认数字证书有效，生成一个新的随机数C（Premaster secret），使用证书中的公钥对C加密，发送给服务端

- **Server Finish**  服务端在接收到客户端传过来的Premaster secret加密数据之后，使用私钥对这段加密数据进行解密，并对数据进行验证，也会使用跟客户端同样的方式生成**会话密钥**，一切准备好之后，会给客户端发送一个ChangeCipherSpec（加密算法协议），告知客户端已经切换到协商过的加密套件状态，准备使用加密套件和session secret（**会话密钥**）加密数据了。之后，服务端也会使用session secret加密后一段Finish消息发送给客户端，以验证之前通过握手建立起来的加解密通道是否成功。

  根据之前的握手信息，如果客户端和服务端都能对Finish信息进行正常加解密且消息正确的被验证，则说明握手通道已经建立成功，接下来，双方可以使用上面产生的session secret对数据进行加密传输了。

> 客户端和服务器根据约定的加密方法，使用三个随机数ABC，生成对话秘钥，之后的通信都用这个对话秘钥进行加密。
>
> TLS 详解 https://juejin.cn/post/6844903667577929742



![image](https://gitee.com/a2547555298/imags/raw/master/ssl.png)

**RSA算法**

**DH算法的握手**

DH也是一种非对称加密算法

![img](http://www.ruanyifeng.com/blogimg/asset/2014/bg2014092007.png)

第三步和第四步由传递Premaster secret变成了传递DH算法所需的参数，然后双方各自算出Premaster secret。

![image-20210224234752296](https://gitee.com/a2547555298/imags/raw/master/image-20210224234752296.png)

共享明文参数P，G，双方都产生一个随机数A，B私钥。

[深入理解https工作原理](https://mp.weixin.qq.com/s/G-mw8oJHWgcSOAV8Vp6bPw)

### QUIC

QUIC(Quick UDP Internet Connection)是谷歌推出的一套基于 **UDP** 的传输协议，它实现了 TCP + HTTPS + HTTP/2 的功能，目的是**保证可靠性的同时降低网络延迟。**HTTP/3.0 就将基于 QUIC 协议实现，也就是 HTTP over QUIC。

##### 为什么选择 UDP

TCP 协议存在如下的问题：

- **网络环境的不适应**：TCP 协议在网络流行的早期推出，因为那时的网络不像如今这样发达，丢包率十分之高，因此非常迫切地需要一款协议来保证数据的可靠传输。但如今的网络环境已经与当年大不相同了，相对来说非常的可靠，对于 TCP 中的一部分通过性能的损耗而保证可靠性的机制，对我们来说不再是必要的，我们更需要网络协议能带来更高的性能。
- **TCP 的更新成本过高**：TCP 协议中的一些对网络的优化其实并不是最优解，在目前的角度看来实际上仍有优化的空间。但如果要修改 TCP 协议，它的代价非常之大，因为它的实现往往存在于操作系统的内核中，要对它进行修改意味着需要依赖操作系统内核的更新。

#### **连接迁移**

##### **tcp 的连接重连之痛**

TCP 连接基于四元组（源 IP、源端口、目的 IP、目的端口），切换网络时至少会有一个因素发生变化，导致连接发生变化。当连接发生变化时，如果还使用原来的 TCP 连接，则会导致连接失败，就得等原来的连接超时后重新建立连接。

QUIC 的连接不受四元组的影响，当这四个元素发生变化时，原连接依然维持。因为QUIC 是基于 UDP 协议的， QUIC 连接不以四元组作为标识。而是使用一个 64 位的随机数，这个随机数被称为 Connection ID，即使 IP 或者端口发生变化，只要 Connection ID 没有变化，那么连接依然可以维持，**上层业务逻辑感知不到变化，不会中断，也就不需要重连。**。

这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。

#### **低连接延时**

##### **TLS 的连接时延问题**

1. TCP 握手需要 1 个 RTT；
2. TLS 握手，以目前应用最广泛的 TLS 1.2 而言，需要 2 个 RTT。对于非首次建连，可以选择启用会话重用，则可缩小握手时间到 1 个 RTT；
3. HTTP 业务数据交互，假设 abc.com 的数据在一次交互就能取回来。那么业务数据的交互需要 1 个 RTT；经过上面的过程分析可知，要完成一次简短的 HTTPS 业务数据交互，需要经历：新连接 **4RTT + DNS**；会话重用 **3RTT + DNS**。

所以，对于数据量小的请求而言，单一次的请求握手就占用了大量的时间，对于用户体验的影响非常大。同时，在用户网络不佳的情况下，RTT 延时会变得较高，极其影响用户体验。

![图片](https://gitee.com/a2547555298/imags/raw/master/tls%E6%8F%A1%E6%89%8B.png)

##### **真·0-RTT 的 QUIC 握手**

QUIC 由于基于 UDP，无需 TCP 连接，在最好情况下，短连接下 QUIC 可以做到 0RTT 开启数据传输。而基于 TCP 的 HTTPS，即使在最好的 TLS1.3 的 early data 下仍然需要 1RTT 开启数据传输。

QUIC 具体握手过程如下：

**Step1**：首次连接时，客户端发送 Inchoate Client Hello 给服务端，用于请求连接；

**Step2**：服务端生成 g、p、a，根据 g、p 和 a 算出 A，然后将 g、p、A 放到 Server Config 中再发送 Rejection 消息给客户端；

**Step3**：客户端接收到 g、p、A 后，自己再生成 b，根据 g、p、b 算出 B，根据 A、p、b 算出初始密钥 K。B 和 K 算好后，客户端会用 K 加密 HTTP 数据，连同 B 一起发送给服务端；

**Step4**：服务端接收到 B 后，根据 a、p、B 生成与客户端同样的密钥，再用这密钥解密收到的 HTTP 数据。为了进一步的安全（前向安全性），服务端会更新自己的随机数 a 和公钥，再生成新的密钥 S，然后把公钥通过 Server Hello 发送给客户端。连同 Server Hello 消息，还有 HTTP 返回数据；

**Step5**：客户端收到 Server Hello 后，生成与服务端一致的新密钥 S，后面的传输都使用 S 加密。

这样，**QUIC 从请求连接到正式接发 HTTP 数据一共花了 1 RTT**（获取服务端Config后就可以携带数据发送了），这 1 个 RTT 主要是为了获取 Server Config，**后面的连接如果客户端缓存了 Server Config，那么就可以直接发送 HTTP 数据**，实现 0 RTT 建立连接。

> 双方更换为使用会话密钥 S通信，初始密钥 K此时已无用，QUIC 握手过程完毕。之后会话密钥 S更新的流程与以上过程类似，只是数据包中的某些字段略有不同。
>
> step4中只是再次通过DH算法协商生成新的密钥S代替老的密钥K。根据文中描述是为了前向安全性：
>
> “前向安全或前向保密（英语：Forward Secrecy，缩写：FS），是密码学中通讯协议的安全属性，指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。 [2] 前向安全能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁。如果系统具有前向安全性，就可以保证在主密钥泄露时历史通讯的安全，即使系统遭到主动攻击也是如此。“





![图片](https://gitee.com/a2547555298/imags/raw/master/quic%E6%8F%A1%E6%89%8B.png)

![img](https://pic2.zhimg.com/v2-8e8cb95703a58fe465d753a57e379e49_b.jpg)

这里使用的是 DH 密钥交换算法，DH 算法的核心就是服务端生成 a、g、p 3 个随机数，a 自己持有，g 和 p 要传输给客户端，而客户端会生成 b 这 1 个随机数，**通过 DH 算法客户端和服务端可以算出同样的密钥**。在这过程中 a 和 b 并不参与网络传输，安全性大大提高。因为 p 和 g 是大数，所以即使在网络中传输的 p、g、A、B 都被劫持，那么靠现在的计算机算力也没法破解密钥。

#### **可自定义的拥塞控制**

Quic 使用可插拔的拥塞控制，相较于 TCP，它能提供更丰富的拥塞控制信息。比如对于每一个包，不管是原始包还是重传包，都带有一个新的序列号(seq)，这使得 Quic 能够区分 ACK 是重传包还是原始包**，从而避免了 TCP 重传模糊的问题（计算rtt)**。Quic 同时还带有收到数据包与发出 ACK 之间的时延信息。这些信息能够帮助更精确的计算 rtt。此外，Quic 的 ACK Frame 支持 256 个 NACK 区间，相比于 TCP 的 SACK(Selective Acknowledgment)更弹性化，更丰富的信息会让 client 和 server 哪些包已经被对方收到。

QUIC 的传输控制不再依赖内核的拥塞控制算法，而是实现在**应用层**上，**这意味着我们根据不同的业务场景，实现和配置不同的拥塞控制算法以及参数。**GOOGLE 提出的 BBR 拥塞控制算法与 CUBIC 是思路完全不一样的算法，在弱网和一定丢包场景，BBR 比 CUBIC 更不敏感，性能也更好。在 QUIC 下我们可以根据业务随意指定拥塞控制算法和参数，甚至同一个业务的不同连接也可以使用不同的拥塞控制算法。

#### **无队头阻塞**

##### **TCP 的队头阻塞问题**

虽然 HTTP2 实现了多路复用，但是因为其基于面向字节流的 TCP，因此一旦丢包，将会影响多路复用下的所有请求流。QUIC 基于 UDP，在设计上就解决了队头阻塞问题。

TCP 队头阻塞的主要原因是数据包超时确认或丢失阻塞了当前窗口向右滑动，QUIC解决队头阻塞的方案是**不让**超时确认或丢失的数据包将当**前窗口阻塞在原地。**

TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达。

![图片](https://gitee.com/a2547555298/imags/raw/master/HTTP2对头阻塞)



如上图，应用层可以顺利读取 stream1 中的内容，但由于 stream2 中的第三个 segment 发生了丢包，**TCP 为了保证数据的可靠性，需要发送端重传第 2个 segment 才能通知应用层读取接下去的数据。**所以即使 stream3 stream4 的内容已顺利抵达，应用层仍然无法读取，只能等待 stream2 中丢失的包进行重传。

在弱网环境下，HTTP2 的队头阻塞问题在用户体验上极为糟糕。

##### **QUIC 的无队头阻塞解决方案**

QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 Sequence Number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值，比如 Packet N+M。

QUIC 使用的 Packet Number 单调递增的设计**(疑问：序列化也可以指明位置，为啥要有序确认）**，**可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认**，当数据包 Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会**继续向右滑动**。待发送端获知数据包 Packet N 丢失后，会将需要重传的数据包放到待发送队列，重新编号比如数据包 Packet N+M 后重新发送给接收端，对重传数据包的处理跟发送新的数据包类似，这样就不会因为丢包重传将当前窗口阻塞在原地，从而解决了队头阻塞问题。那么，既然重传数据包的 Packet N+M 与丢失数据包的 Packet N 编号并不一致，我们怎么确定这两个数据包的内容一样呢？

QUIC 使用 **Stream ID** 来标识当前数据流属于哪个资源请求，这同时也是数据包多路复用传输到接收端后能正常组装的依据。重传的数据包 Packet N+M 和丢失的数据包 Packet N 单靠 Stream ID 的比对一致仍然不能判断两个数据包内容一致，还需要再新增一个字段 **Stream Offset**，标识当前数据包在当前 Stream ID 中的字节偏移量。

**有了 Stream Offset 字段信息，属于同一个 Stream ID 的数据包也可以乱序传输了**（HTTP/2 中仅靠 Stream ID 标识，要求同属于一个 Stream ID 的数据帧必须有序传输），通过两个数据包的 Stream ID 与 Stream Offset 都一致，就说明这两个数据包的内容一致。

![图片](https://gitee.com/a2547555298/imags/raw/master/%20QUIC%E6%97%A0%E9%98%9F%E5%A4%B4%E9%98%BB%E5%A1%9E.png)

超时发生后客户端发起重传，后来接收到了 ACK 确认消息，但因为原始请求和重传请求接收到的 ACK 消息一样，所以客户端不知道这个 ACK 对应的是原始请求还是重传请求。如果客户端认为是原始请求的 ACK，但实际上是左图的情形，则计算的采样 RTT 偏大；如果客户端认为是重传请求的 ACK，但实际上是右图的情形，又会导致采样 RTT 偏小。图中有几个术语，RTO 是指超时重传时间（Retransmission TimeOut），跟我们熟悉的 RTT（Round Trip Time，往返时间）很长得很像。采样 RTT 会影响 RTO 计算，超时时间的准确把握很重要，长

QUIC 解决了上面的歧义问题。Packet Number严格单调递增，这样发送方接收到确认消息时就能方便地知道 ACK 对应的是原始请求还是重传请求。

![img](https://gitee.com/a2547555298/imags/raw/master/v2-f1847d49c3bd83cd4c51e337dd40af7b_b.jpg)



**QUIC 协议组成**

QUIC 的 packet 除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。

如图 3-1 所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。

![图片](https://gitee.com/a2547555298/imags/raw/master/QUIC%E7%9A%84%E5%8D%8F%E8%AE%AE%E7%BB%84%E6%88%90.png)

- **Flags**: 用于表示 Connection ID 长度、Packet Number 长度等信息；
- **Connection ID**：客户端随机选择的最大长度为 64 位的无符号整数。但是，长度可以协商；
- **QUIC Version**：QUIC 协议的版本号，32 位的可选字段。如果 Public Flag & FLAG_VERSION != 0，这个字段必填。客户端设置 Public Flag 中的 Bit0 为 1，并且填写期望的版本号。如果客户端期望的版本号服务端不支持，服务端设置 Public Flag 中的 Bit0 为 1，并且在该字段中列出服务端支持的协议版本（0 或者多个），并且该字段后不能有任何报文；
- **Packet Number**：长度取决于 Public Flag 中 Bit4 及 Bit5 两位的值，最大长度 6 字节。发送端在每个普通报文中设置 Packet Number。发送端发送的第一个包的序列号是 1，随后的数据包中的序列号的都大于前一个包中的序列号；
- **Stream ID**：用于标识当前数据流属于哪个资源请求；
- **Offset**：标识当前数据包在当前 Stream ID 中的字节偏移量。



相比 HTTPS：

1. 减少了 TCP 的三次握手以及 TLS 握手的时间，相比 HTTPS 的 3RTT 握手，可以实现 0RTT 握手。
2. 引入 QUIC 连接及 Stream 的概念，对 HTTP/2 对多路复用进行了改进，每条 Stream 对应一个 HTTP 请求，Stream 之间互不影响。
3. 对拥塞控制机制进行了改进，实现了 TCP 的拥塞控制算法并引入了更多如 BBR 等算法，同时由于位于应用层，修改、配置更为方便。
4. RTT 计算更为精准，PacketNumber 单调递增解决了 TCP 的重传导致的 RTT 计算歧义性，并且 RTT 的计算考虑了 ACK 延迟机制。
5. 通过 Connection Id 唯一标示了一条 QUIC 连接，从而实现两端的 IP 等信息变化仍能维持同一条 QUIC 连接。
6. 通过引入 Stream Offset 字段实现了一条 Stream 上的按序传输的能力。
7. 引入了前向纠错机制，实现了纠错功能。



HTTP/3 原理实战 - 腾讯技术工程的文章 - 知乎 https://zhuanlan.zhihu.com/p/143464334

### DNS

DNS是提供**域名到IP地址**间转换的分布式系统，是**应用**层协议。通常运行在**UPD**协议之上，端口号**53**。

**域名**

域名是一种分层的结构， `com`、`cn`、`org`、`net` 等等就是顶层域名，每一级域名需要由上一级域名进行分配。

**DNS服务器**

DNS服务器主要分为根域名服务，顶级域名服务器，权威域名服务器。

还有一种不属于DNS层次结构的本地DNS服务器，仅是用来转发DNS请求，将查询结果缓存在本地。用来提供跟快捷的DNS查询，往往由ISP维护，本地DNS服务器还维护了到每个根域名服务器的IP地址。这个本地 DNS 服务器的地址是由我们用户所配置在操作系统中的。

**递归查询和迭代查询**

递归查询：如果该DNS服务器没有缓存的DNS信息，则会**自己向其他服务器进行询问**，获得结果后告诉客户端

迭代查询：如果在DNS服务器上没有对应的DNS信息，则会告诉查询方能够查询到该域名的DNS服务器，然后**客户端再自己向返回的服务器进行查询。**

**解析过程**

**递归查询**：基于缓存实现

- 浏览器缓存
- **系统缓存**：查看系统的 Hosts 文件的 DNS 缓存查看是否有对应的映射。
- 路由器缓存
- ISP DNS服务器

当本地DNS服务器也没有缓存时，会将请求发给根服务器开始迭代查询。

**迭代查询**

- 根服务器，根服务器会对区域数据进行查询，若没有则将对应的顶级域名DNS服务器告诉本地DNS服务器
- 顶级域名服务器
- 权威域名服务器
- 下一级域名服务器
- 依次往下进行迭代**直到**查找到映射

**存入缓存**：*本地 DNS 服务器*会将结果保存到缓存，以备下次使用。

**DNS实现负载平衡**

一般大型网站使用多个服务器提供服务，因此一个域名可能对于多个IP地址

- 当用户发起DNS请求时，DNS返回对应服务器IP地址集合
- 在每个应答中，循环这些IP地址的顺序，因为用户一般选择靠前的IP
- 以此将用户请求均衡到各个不同服务器上，实现负载均衡

#### 常见问题

**DNS为什么使用UDP作为传输层协议**

- 为了得到一个域名的IP地址，往往会向多个域名服务器查询。如果使用TCP，每次请求都存在连接时延，使DNS服务变慢。
- DNS追求速度，如果UDP发生丢包重新发一个请求即可

**什么时候用到TCP**

1. UDP报文不足于承载对应的响应（UDP限制最大为512个字节）
2. 进行DNS区域传输（对DNS数据进行迁移），要保证可靠的数据交互

**为什么域名根服务器只能有13台呢？**

DNS 消息使用 UDP 协议进行传输，这规定了消息最大的长度在 512 字节（不包含 IP 头部、UDP 头部）。

有了最大长度限制之后，一个 UDP 协议传输的 DNS 响应能够返回的资源记录数量就是有限的。

**要让所有的根服务器数据能包含在一个 512 字节的 UDP 包中，根服务器只能限制在 13 个。**

### Websocket

####Websocket和HTTP区别

HTTP协议和WebSocket协议都是应用层的协议，两者**应用场景不一样。** 

HTTP主要用来**请求-应答**的方式交付信息，一个request对应一个response；HTTP这种**单向请求**的特点，注定了如果**服务器有连续的状态变化**，客户端要获知就非常麻烦。

**几种与服务端实时通信的方法**

> 不使用WebSocket与服务器实时交互，一般有两种方法。AJAX轮询和Long Polling长轮询。
>
> **AJAX短轮询**
>
> AJAX轮询也就是定时发送请求，可以保证服务端一旦有最新消息，就可以被客户端获取。
>
> 缺点是：当客户端以固定频率向服务器端发送请求时，服务器端的数据可能并没有更新，**带来很多无谓请求**，浪费带宽，效率低下。
>
> **Long Polling长轮询**
>
> Long Polling长轮询是客户端和浏览器保持一个长连接。
>
> 客户端发起一个Long Polling，服务端如果没有数据要返回的话，
> 会阻塞住请求，等到有数据，就会返回给客户端并关闭连接。客户端处理完响应信息后，客户端又会再次发起一次Long Polling，再重复一次上面的过程。
>
> **缺点**
>
> 虽然长轮询和短轮询比起来，它的优点是**「明显减少了很多不必要的 http 请求次数」**，相比之下节约了资源。但当 server 向 client 发送数据后，必须等待下一次请求才能将新的数据发送出去，这样 client 接收到新数据的间隔最短时间便是 2 * RTT（往返时间），这样便无法应对 server 端数据更新频率较快的情况。
>
> **两种都是都是在不断地建立HTTP连接，然后等待服务端处理，可以体现HTTP协议的另外一个特点，**被动性**。**

HTTP2虽然支持服务器推送资源到客户端，但那不是应用程序可以感知的，主要是让浏览器（用户代理）**提前缓存静态资源**，所以我们不能指望HTTP2可以像WebSocket建立双向实时通信。

WebSocket让通信双方都可以主动去交换信息。

**Websocket 和 HTTP 什么关系？**

HTTP、WebSocket 都是应用层协议，都是基于 TCP 协议来传输数据的。

#### Websocket简介

它的最大特点就是为了提供 Web 应用程序和服务端**全双工通信**而专门制定的，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，**是真正的双向平等对话**。

其他特点包括：

- **建立在 TCP 协议之上**，服务器端的实现比较容易（监听socket流）。
- 与 HTTP 协议有着良好的兼容性。默认端口也是80（ws)和443(wss)，并且**握手阶段采用 HTTP 协议**建立连接，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。
- **数据格式比较轻量**，**性能开销小**，节省带宽。客户端与服务端进行数据交换时，服务端到客户端的数据包头   只有2到10字节，客户端到服务端需要加上另外4字节的掩码。
- 更好的二进制支持，可以发送文本，和二进制数据
- 协议标识符是`ws`（如果加密，则为`wss`），服务器网址就是 URL。

> ```markup
> ws://example.com:80/some/path
> ```

![img](https://gitee.com/a2547555298/imags/raw/master/bg2017051503.jpg)

连接建立后，WebSocket 采用二进制帧的形式传输数据，其中常用的包括用于数据传输的**数据帧 MESSAGE** 以及 **3 个控制帧：**

- PING：主动保活的 PING 帧
- PONG：收到 PING 帧后回复
- CLOSE：主动关闭 WebSocket 连接；

WebSocket 的保活，需要定时发送 PING 帧

#### **WebSocket连接过程**

为了创建Websocket连接，需要通过浏览器发出请求，之后服务器进行回应，这个过程通常称为“[握手](https://baike.baidu.com/item/握手)”。

为了兼容性客户端使用**HTTP发起握手**，告诉服务端进行WebSocket协议通讯，并告知WebSocket协议版本。服务端确认协议版本，升级为WebSocket协议， 如果服务器或者代理不支持 WebSocket，它们会把这当做一个不认识的 HTTP 请求地拒绝掉。

相比于传统 HTTP 的每次“请求-应答”都要 client 与 server 建立连接的模式，WebSocket 是一种长连接的模式。

经过**一次HTTP请求**握手成功后，数据就直接从 TCP 通道传输，与 HTTP 无关了。就可以做到源源不断的信息传送了。之后如果有数据需要推送，会主动推送给客户端。

连接开始时，客户端使用HTTP协议和服务端升级协议，升级完成后，后续数据交换遵循WebSocket协议。**Request Headers**

```json
Accept-Encoding: gzip, deflate, br
Accept-Language: zh,zh-TW;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6
Cache-Control: no-cache
Connection: Upgrade
Host: 127.0.0.1:3000
Origin: http://localhost:3000
Pragma: no-cache
Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits
Sec-WebSocket-Key: bwb9SFiJONXhQ/A4pLaXIg==
Sec-WebSocket-Version: 13
Upgrade: websocket
```

重点字段是这些：

- Connection: Upgrade 表示要升级协议
- Upgrade: websocket 要升级协议到websocket协议
- Sec-WebSocket-Version 表示websocket的版本。如果服务端不支持该版本，需要返回一个Sec-WebSocket-Versionheader，里面包含服务端支持的版本号。
- Sec-WebSocket-Key 对应服务端响应头的Sec-WebSocket-Accept，由于没有同源限制，websocket客户端可任意连接支持websocket的服务。这个就相当于一个钥匙一把锁，避免多余的，无意义的连接。

再看看看服务端响应的 Response Headers

```json
HTTP/1.1 101 Switching Protocols
Connection: Upgrade
Sec-WebSocket-Accept: 2jrbCWSCPlzPtxarlGTp4Y8XD20=
Upgrade: websocket
```

关键是这个字段

- Sec-WebSocket-Accept: 用来告知服务器愿意发起一个websocket连接， 值根据客户端请求头的Sec-WebSocket-Key计算出来

**WebSocket如何实现长链接**

- TCP是持久连接，建立TCP连接是3次握手，关闭TCP连接是4次挥手。TCP连接是由通信双方来决定什么时候结束通信，那么自然就是一个持久连接。TCP连接可以进行全双工通信，因为双方都知道对方是谁。WebSocket协议本身就是针对于全双工通信设计的，通信双方都可以发起/响应请求
- Http协议只能单向通信的原因是：Server端没有保存Http客户端的信息，想要通信的时候找不到客户端。（只有URL信息）
- WebSocket如何管理连接的呢？协议定义了Control Frame。WebSocket的控制帧有：Close、Ping、Pong。其中Close发起关闭请求；Ping帧是通信发起方确认链路是否畅通的报文；Pong则是通信接收方回应链路是否畅通的报文。

#### **Socket 与 WebSocket 的关系**

Socket 并不是一个协议，是为了方便大家直接使用更底层协议，在应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。

在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。

~~主机 A 的应用程序要能和主机 B 的应用程序通信，必须通过 Socket 建立连接，而建立 Socket 连接必须需要底层 TCP/IP 协议来建立 TCP 连接。建立 TCP 连接需要底层 IP 协议来寻址网络中的主机。我们知道网络层使用的 IP 协议可以帮助我们根据 IP 地址来找到目标主机，但是一台主机上可能运行着多个应用程序，如何才能与指定的应用程序通信就要通过 TCP 或 UPD 的地址也就是端口号来指定。这样就可以**通过一个 Socket 实例唯一代表一个主机上的一个应用程序的通信链路了**~~

而 WebSocket 则不同，它是一个完整的[应用层协议](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttp%3A//datatracker.ietf.org/doc/rfc6455/)，包含一套[标准的 API](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttp%3A//dev.w3.org/html5/websockets/)。所以，从使用上来说，WebSocket 更易用，而 Socket 更灵活。

### TCP

TCP 在 IP 协议不可靠（尽力而为也就是无服务）之上建立的**可靠的全双工字节流数据传输服务**

- 基于**字节流**：以字节为单位对传输数据进行处理
- **全双工**：一条TCP连接上的两端可以互相发送信息
- **可靠的：**通过TCP连接传输的数据不丢失，无差错，不重复，按序到达
- **面向连接：** 使用TCP传输数据前必须先建立连接，传输完成后释放连接

#### **报文结构**

- 报文段= 首部 + 数据
- 固定首部20字节

![](https://gitee.com/a2547555298/imags/raw/master/TCP%E6%8A%A5%E6%96%87%E6%AE%B5.png)

**一个 TCP 连接（TCP Socket）可以被`源 IP 地址`、`源端口号`、`目的 IP 地址`、`目的端口号`这样一个四元组所唯一确定**。

- 序号：这个报文段首字节的字节编号，其中数据流的初始序号是随机选择的
- 确认号：接收方期望从发送方收到的下一个字节编号
- 数据偏移：首部长度
- URG：存在紧急字段，紧急指针有效
- PSH：接收方应立即将数据交给上层
- RST：异常的关闭连接
- ACK：指示确认号有效
- SYN：发起连接请求时SYN=1，ACK=0；对方同意建立连接时，SYN=1，ACK=1
- FIN:为1释放连接
- 窗口：用于流量控制，表示接收方愿意接收的字节数量
- 校验和：用以校验首部和数据

#### **连接建立**

![image-20200222162116971](https://gitee.com/a2547555298/imags/raw/master/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.jpg)

1. 客户端向服务端发送一个SYN字段为1的SYN报文段，并随机选择一个初始序号X，填入序号字段。随机的目的是安全性
2. 服务端收到SYN报文段后，会向客户端发送一个SYNACK报文段。SYNACK 报文段不包含数据，其 SYN 字段、ACK 字段均为 1，且将确认号置为了 X+1（表示下一个想接收这个序号的报文段），并且也会生成一个**随机**的初始序号（Y）并填入序号字段。
3. 客户端收到 SYNACK 报文段后会为该连接分配缓存，并向服务端发送一个 **ACK 报文段**，其 ACK 字段为 1，并将序号置为了X+1，将确认号置为了 Y+1。当服务端收到该 ACK 报文后，则会为客户端的连接分配缓存。（这个步骤的 ACK 报文是可以携带数据的）

初始序号 ISN 设置为随机主要是为了安全考虑，因为如果均为一个固定的值，那么恶意者可以通过这个已知的序号来**伪装**发送方或接收方的其中一方，并造成一些破坏。

**为什么是三次握手，而不是四次或两次**

1. **防止已失效的请求报文到达服务端**，服务端错误建立连接浪费资源
2. 三次握手才能**协商好序号**保证有序传输，如果是两次的只能确认发送方的seq
3. 如果进行四次握手，显然第二步的发送 ACK 与发送 SYN 报文段的过程在数据来说没有交叉，并且时间几乎连续，因此是可以合并的，于是便成为了三次握手。

**三次握手中 SYN 丢包如何处理**

- 客户端会**超时重传**，超过设定的次数仍未收到 ACK 则结束

**三次握手中 SYNACK 丢包如何处理**

- 客户端以为SYN发生丢包，对SYN进行重传
- 服务端也会触发重传SYNACK包

**三次握手中最后 ACK 丢包如何处理**

- 服务端收不到 ACK 会进行重传，此时客户端会认为连接已建立，处于 ESTABLISHED，而服务端仍处于 SYN_RCVD，此时如果服务端收到了客户端的数据包，会认为连接已建立，进入 ESTABLISHED。

#### **连接关闭（四次挥手）**

![](https://gitee.com/a2547555298/imags/raw/master/TCP关闭连接)

- 客户端向服务端发送一个 FIN 报文段，表示客户端想要关闭向服务端的数据流向，不再向服务端发送数据。
- 服务端收到该 FIN 报文段后，会向客户端发送一个 ACK 报文段，对该连接的关闭进行了确认，并回收为其分配的资源。
- 服务端向客户端发送一个 FIN 报文段，表示服务端想要关闭向客户端的数据流向，不再向客户端发送数据。
- 客户端收到该 FIN 报文段后，会向服务端发送一个 ACK 报文段，对该连接的关闭进行了确认，并回收为其分配的资源。
- 客户端等待**2MSL**后正式关闭

在客户端向服务端发送 FIN 并收到 ACK 后，仅仅是客户端向服务端发送数据的这一条数据流向被关闭，**服务端仍然能继续向客户端发送数据**。这种连接一端结束发送后还能接收另一端数据的能力叫做**半关闭**。

**为什么不是三次挥手**

因为TCP是全双工模式，当服务端收到客户端的FIN报文时，**仅**代表客户端向服务端停止发送数据。但服务端可能仍有数据没有发送完，还不能关闭服务端到客户端的数据流。**因此服务端发送的 ACK 与发送的 FIN 的时机不一定是相同的**，可能会有一段较长的时间间隔，无法将两条数据合并为同一条。

==<u>为什么要等待**2MSL**？</u>== 

MSL：报文段最大生存时间，报文段被丢弃前在网络内的最长时间。 

- 保证TCP协议的全双工连接能够**可靠关闭** 
- 保证上次连接的数据段从**网络中消失** 

#### 流量控制

如果发送方把数据发送得过快，**接收方可能会来不及接收**，这就会造成数据的丢失。**流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。** 

TCP 通过大小可变的**滑动窗口**来实现流量控制，由于 TCP 是全双工的协议，因此两端都可以作为发送端以及接收端。因此 TCP 连接的两端都具有一个**发送窗口**（`swnd`）和一个**接收窗口**（`rwnd`）。

**TCP保证可靠，每一个数据包都要确认。**有了滑动窗口，接收方可以等收到许多包后只发送一个ACK包，确认之前收到的多个数据包。发送方在**滑动窗口大小内**，发完一个数据包后不用等待ACK，继续发送其他数据包

**TCP为每一个连接设有一个持续计时器**(persistence timer)。当TCP连接中的发送方收到接收方的零窗口通知时，发送方就启动持续计时器。若**持续计时器设置的时间** **到期**，发送方就发送一个**零窗口控测报文段**（携1字节的数据）给接收方。如果接收方可以接收数据，就重新开始发送数据；如果接收方不能接收数据，就重新设置持续计时器。

####拥塞控制 

流量控制是发送方和接收方之间的控制。拥塞控制是一个**全局性**的过程，对网络的拥塞情况进行控制。

> 拥塞：对网络中的资源需求 > 该资源所能提供的部分

TCP 同样是通过大小可变的滑动窗口来实现拥塞控制，TCP 的每一端都具有一个**拥塞窗口**（`cwnd`）。

发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些， 以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 

- **慢开始算法**
  - 拥塞窗口 cwnd 设置为**一个最大报文段MSS（Maximum Segment Size，最大报文长度）的数值。**
  - 每收到一个确认，拥塞窗口大小+1个MSS。
  - 每经过一个传输轮次，拥塞窗口就加倍
  - 慢指的是一开始发送报文段时拥塞窗口`（cwnd）`设置得较小（为1MSS），目的是试探一下网络的拥塞情况
- **拥塞避免**
  - 当 cwnd > 慢开始门限ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 
  - 使得拥塞窗口`（cwnd）`**按线性规律 缓慢增长**：每经过一个往返时间`RTT`，发送方的拥塞窗口`（cwnd）`加1

无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络**出现拥塞**（其根据就是没有收到确认），**就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）**。**然后把拥塞窗口cwnd重新设置为1**，重新开始慢开始算法。

目的：

- **迅速减少**主机发送到网络中的分组数
- 使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。  

![img](https://gitee.com/a2547555298/imags/raw/master/944365-588901fb01c9aea2.png)



- **快重传** 

  - 发送方只要一连收到3个重复确认就立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器到期。**就不会因为超时，误以为网络堵塞开始慢开始算法**

- 快恢复

  当发送方连续收到3个重复确认后，就：

  - 把 慢开始门限`（ssthresh）`设置为 出现拥塞时发送方窗口值的一半 = 拥塞窗口的1半
  - 将拥塞窗口`（cwnd）`值设置为 慢开始门限`ssthresh`减半后的数值
  - 执行拥塞避免算法，使拥塞窗口缓慢地线性增大。

![img](https://gitee.com/a2547555298/imags/raw/master/%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6.png)



#### 丢包问题

TCP是基于不可靠的网络实现可靠传输，可能会存在丢包问题。

为了满足TCP协议不丢包。TCP协议有如下规定：

1. 数据分块：发送端对数据进行分块（TCP基于字节流），接受端要对数据进行重组，由TCP**根据滑动窗口值和拥塞情况**确定分块的大小并控制分块和重组

2. 到达确认：接收端接收到分片数据时，根据分片数据序号向发送端发送一个确认

3. 超时重发：发送方在发送分片时设置超时定时器，如果在定时器超时之后没有收到相应的确认，重发分片数据

4. 滑动窗口：TCP连接的每一方的接受缓冲空间大小固定，接收端只允许另一端发送接收端缓冲区所能接纳的数据，TCP在滑动窗口的基础上提供流量控制，防止较快主机致使较慢主机的缓冲区溢出

5. 失序处理：作为IP数据报来传输的TCP分片到达时可能会失序，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层;

6. 重复处理：作为IP数据报来传输的TCP分块可能会发生重复，TCP的接收端必须丢弃重复的数据;

7. 数据校验：TCP将保持它首部和数据的检验和，这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到分片的检验或有差错，TCP将丢弃这个分片，并不确认收到此报文段导致对端超时并重发

#### 合并优化算法

**Nagle 算法**

核心思想就是：**TCP 连接上只能存在一个未被确认的小分组，其余的分组在该分组 ACK 未到达前不能发送**。这些小分组会被收集起来，在 ACK 到来时合并为一个更大的分组发送出去。

**延迟 ACK**

接收方收到对方的报文时，不会立即发送 ACK，而是进行了一段时间的延迟。

在这段延迟的时间中：

- 如果接收方有**数据**要发送给发送方，则将要发送的数据与 ACK 一块发送。
- 如果接收到了更多对方发来的数据，则只需要将 **ACK 一并发送**（确认最后一个），而不需要发送多个 ACK。

**Nagle 遇上延迟 ACK**

如果发送方发送了多个小分组给接收方，接收方第一次接收到分组的时候，会对 ACK 进行延迟，而发送方在这个 ACK 到达之前会收集其他小分组等待其到达，**造成了一个较大的时延**。因此 **Nagle 算法配合延迟 ACK 的效果不是很理想**。

##### 粘包问题

首先要明确一个概念：**粘包问题属于应用层的问题，不是 TCP 协议的问题，TCP 协议的任务仅仅是负责将应用层的数据正确的传输**。这往往是使用 TCP 时应用不当而导致的问题。

粘包核心原因在于由于 TCP 协议是面向字节流的，因此它的**数据并没有边界**，这就导致接收方收到的数据很可能是几个数据包『粘』在一起的，从而无法对数据进行处理。

解决方法：

1. 标示数据长度以及数据头，服务端获取消息时解析出消息长度，然后向后读取对应长度内容。
2. 将消息设置为定长，若不足则补上固定分隔符。
3. 加入消息边界，服务端按消息边界对消息内容进行划分。

#### 长连接

三次握手建立连接后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。

保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃。如果客户已经消失，使得服务器上保留一个半开放的连接，浪费资源。保活功能就是试图检测到客户端的连接是否正常。

如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一：

1. 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器将保活定时器复位。
2. 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。
3. 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。
4. 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。

### UDP

![](https://gitee.com/a2547555298/imags/raw/master/UDP%E6%8A%A5%E6%96%87.png)



**只需要 `目的 IP 地址`、`目的端口号` 这样一个二元组即可唯一确定一个 UDP Socket**。服务器上对应的进程，不在乎你是从哪个客户端来的，我都放进同一个套接字处理，处理完了再根据源端口号和源 IP 地址，把应答信息发送给客户端。

- **特点**：无连接、面向报文、不可靠、无序、速度快、轻量、实时性高（无队首阻塞）
- **适用场景**：适用于一对多、即时通讯、视频通话等

**伪首部**

伪首部的数据都是从IP数据报头获取的，其目的是让检查数据是否已**经正确到达目的地**

![](https://gitee.com/a2547555298/imags/raw/master/006tNbRwgy1ga46xxgk9vj30j90b6t9n.jpg)

**使用场景**

##### TCP 与 UDP 的对比

- TCP 是**面向字节流**的，它的数据是没有边界的。而 UDP 是**面向报文**的，它的数据是有边界的。

- TCP 协议可以**保证数据的按序、可靠交付**，UDP 无法保证数据的**可靠交付**。

- TCP 协议是面向连接的，在发送数据前要通过三次握手建立连接，关闭连接需要四次挥手，而 UDP 不需要。

- TCP Socket 需要通过 `目的、源 IP` 以及 `目的、源端口号` 这个四元组唯一确定，而 UDP Socket 只需要通过 `目的 IP` 以及 `目的端口号` 这个二元组即可唯一确定。

- TCP **为了保证可靠交付丧失了一定的传输效率**，而 UDP 的传输效率更高。

- TCP 的首部在不包含选项字段的情况下有 20 个字节，而 UDP 的首部仅仅有 8 个字节。

- TCP 只支持一对一的数据传输，而 UDP 支持一对一、一对多、多对一的数据传输。

- TCP 具有拥塞控制，可以进尽量避免网络中出现拥塞，而 UDP 则没有拥塞控制，并不会因为中间网络的拥塞而减小占用的带宽，有可能造成加剧网络拥堵。

-  TCP应用场景：

  效率要求相对低，但对准确性要求相对高的场景。因为传输中需要对数据确认、重发、 排序等操作，相比之下效率没有UDP高。举几个例子：文件传输、接受邮件、远程登 录。

  UDP应用场景：

  效率要求相对高，对准确性要求相对低的场景。举几个例子：QQ聊天、在线视频、网络语音电话、广播通信（广播、多播）。

  



### OSI

- 应用层：为应用程序提供服务并规定应用程序通信相关细节
- 表示层：将应用程序的数据转化为适合网络传输格式
- 会话层：负责建立关闭连接，以及数据的分割
- 传输层：进程之间的传输
- 网络层：将数据传输到目的地址，负责寻址和路由选择
- 数据链路层：物理层面上互连节点之间的通信传输，负责帧的生成和接收
- 物理层：负责0,1与电压高低，光的闪灭之间的互换

**优点**：

- 由于使用标准接口，每个通信模块可以**独立开发**，增加自由度，提高生产效率
- **增加代码的重复利用率**，由于通信模块的标准化的外在接口，应用程序可以直接使用现成的通信模块，而无需重新编码，这大大减轻了开发者的负担，间接地提供了生产效率。

![img](https://gitee.com/a2547555298/imags/raw/master/OSI&TCP.png)



###cookie  session  token

因为HTTP是无状态的，每个请求都是全新的，与之前请求无关。但随着出现需要登录的网站，就要管理会话，**记录登录状态**  !!!**重点是登录状态，不是用户信息。**

##### **session** 

在服务端**生成用户相关的 session 数据**，而发给客户端的 **sesssion_id** 存放到 cookie 中，这样用客户端请求时带上 session_id 就可以验证服务器端是否存在 session 数据，以此完成用户认证。

缺点：

- 服务器需要保存每个的**sesssion_id**开销大
- 如果服务器做了负载均衡，那么下一个操作请求到了另一台服务器session会丢失

##### token

在对seesion缺点的改进，可以不保存seesion信息，验证身份。

![image-20210305010853683](https://gitee.com/a2547555298/imags/raw/master/token加密.png)

通过对数据使用只有自己知道的密钥生成签名信息，把这个签名和数据一起作为token ， 由于密钥别人不知道， 就无法伪造token了。

这个token 不保存，当客户端把这个token 发过来的时候，再用同样的HMAC-SHA256 算法和同样的密钥，对数据再计算一次签名，**和token 中的签名做个比较**， 如果相同，我就知道是否**登录**过了，并且可以直接取到user id , 如果不相同， 数据部分肯定被人篡改过， 就告诉发送者：对不起，没有认证。

基于服务器验证方式暴露的一些问题

1. **Seesion：**每次认证用户发起请求时，服务器需要去创建一个记录来存储信息。当越来越多的用户发请求时，内存的开销也会不断增加。
2. **可扩展性：**在服务端的内存中使用Seesion存储登录信息，伴随而来的是可扩展性问题。
3. **CORS(跨域资源共享)：**当我们需要让数据跨多台移动设备上使用时，跨域资源的共享会是一个让人头疼的问题。在使用Ajax抓取另一个域的资源，就可以会出现禁止请求的情况。
4. **CSRF(跨站请求伪造)：**用户在访问银行网站时，他们很容易受到跨站请求伪造的攻击，并且能够被利用其访问其他的网站。

**Token特点：**

1. 无状态

   基于这种无状态和不存储信息，负载负载均衡器能够将用户信息从一个服务传到其他服务器上。

2. 支持移动设备

3. 跨程序调用

   能够与其它程序共享权限的Token。

4. 安全

   请求中发送token而不再是发送cookie能够防止CSRF(跨站请求伪造)。即使在客户端使用cookie存储token，cookie也仅仅是一个存储机制而不是用于认证。不将信息存储在Session中，让我们少了对session操作。

> #####CSRF
>
> ——**CSRF**攻击（比如，用户登陆了网页QQ A，此时用户浏览器端存了一个cookie，网页QQ未下线，然后与此同时登陆了一个钓鱼网站B, 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点qq 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行。 从而B能达到模拟A的权限。
>
>    ——CSRF防御　　
>
> 1. ##### 通过referer、token或者验证码来检测用户提交
>
> 2. ##### 尽量不要在页面的链接中暴露用户隐私信息
>
> 3. ##### 对于用户增删改查操作都用post请求
>
> 4. ##### 避免全站通用的cookie，严格设置cookie的域

Token 中的数据是**明文保存的**， 还是可以被别人看到的， 所以**不能在其中保存像密码**这样的敏感信息。

这样一来， 就不保存session id 了， 只是生成token , 然后验证token ， 用CPU计算时间换取了session 存储空间 ！

基于Token的身份验证的过程如下:

1. 用户通过用户名和密码发送请求。
2. 程序验证。
3. 程序返回一个签名的token 给客户端。
4. 客户端储存token,并且每次用于每次发送请求。
5. 服务端验证token并返回数据。

##### cookie

Cookie是保存在客户端中的，按在客户端中的存储位置，可分为内存Cookie和硬盘Cookie。

- 内存Cookie由浏览器维护，保存在内存中，浏览器关闭后就消失了，其存在时间是短暂的。
- 硬盘Cookie保存在硬盘里，有一个过期时间，除非用户手工清理或到了过期时间，硬盘Cookie不会被删除，其存在时间是长期的。所以，按存在时间，可分为非持久Cookie和持久Cookie。

**cookie的产生**

- 浏览器携带用户认证信息向服务器发起请求
- 服务器认证成功，生成cookie。格式为`key=value`，放入到`Set-Cookie`字段里，随着响应报文发给浏览器。
- 浏览器将`Set-Cookie`字段保存起来，下次请求时会自动将此`key=value`值放入到`Cookie`字段中发给服务端。
- 服务端收到请求报文后，解析cookie，获取用户的身份然后提供个性化的服务。

![img](https://gitee.com/a2547555298/imags/raw/master/cookie产生过程.png)

**Cookie中的参数设置**

说到这里，应该知道了`Cookie`就是服务器委托浏览器存储在客户端里的一些数据，而这些数据通常都会记录用户的关键识别信息。所以`Cookie`需要用一些其他的手段用来保护，防止外泄或者窃取，这些手段就是`Cookie`的属性。

| 参数名   | 作用                                                         | 后端设置方法               |
| -------- | ------------------------------------------------------------ | -------------------------- |
| Max-Age  | 设置cookie的过期时间，单位为秒                               | `cookie.setMaxAge(10)`     |
| Domain   | 指定了Cookie所属的域名                                       | `cookie.setDomain("")`     |
| Path     | 指定了Cookie所属的路径                                       | `cookie.setPath("");`      |
| HttpOnly | 告诉浏览器此Cookie只能用于Http和Https协议传输,禁止其他方式访问 | `cookie.setHttpOnly(true)` |
| Secure   | 告诉浏览器此Cookie只能在Https安全协议中传输,如果是Http则禁止传输 | `cookie.setSecure(true)`   |

如果没有指定Domain的值，那么其Domain的值是默认为当前所提交的http的请求所对应的主域名的。比如访问 http://www.example.com，返回一个cookie，没有指名domain值，那么其为值为默认的www.example.com。

**cookie在安卓端的保存**

构建单例OkhttpClient的时候，设置cookiejar或者拦截器，然后具体的操作（保存Cookie，取Cookie），Okhttp框架就会帮我们自动管理Cookie。我们可以将cookie存储在内存中;

复杂的系统可以使用文件系统用于保存已接受的cookie的数据库。因此，我们就可以通过Map去简单的管理和使用。

**有cookie为什么还要seesion**

1. 用session只需要在客户端保存一个id，实际上大量数据都是保存在服务端。如果全部用cookie，数据量大的时候客户端是没有那么多空间的。
2. cookie只是实现session的其中一种方案。虽然是最常用的，但并不是唯一的方法。
3. 全部在客户端保存，服务端无法验证，这样伪造和仿冒会更加容易。（伪造一个随机的id很难，但伪造另一个用户名是很容易的）
4. 全部保存在客户端，那么一旦被劫持，全部信息都会泄露
5. 客户端数据量变大，网络传输的数据量也会变大

#### 总结

Cookie是存储在客户端的

Session是存储在服务端的，可以理解为一个状态列表。拥有一个唯一会话标识`SessionId`。可以根据`SessionId`在服务端查询到存储的信息。

Session会引发一个问题，即后端多台机器时Session共享的问题，解决方案可以使用Spring提供的框架。

Token类似一个令牌，无状态的，服务端所需的信息被Base64编码后放到Token中，服务器可以直接解码出其中的数据。

## 操作系统

### 概念

#### 操作系统的四大特性

- 并发：同一段时间内多个程序执行
- 共享：系统中的资源可以被内存中多个并发执行的进线程共同使用
- 虚拟：通过时分复用（如分时系统）以及空分复用（如虚拟内存）技术实现把一个物理实体虚拟为多个
- 异步：系统中的进程是以走走停停的方式执行的，且以一种不可预知的速度推进

#### 操作系统基本功能

- 进程管理
  - 进程控制、进程同步、进程通信、死锁处理、处理机调度等
- 内存管理
  - 内存分配、地址映射、内存保护与共享、虚拟内存等
- 文件管理
  - 文件存储空间的管理、目录管理、文件读写管理和保护等
- 设备管理
  - 完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。主要包括缓冲管理、设备分配、设备处理、虛拟设备等

#### CPU调度

- 高级调度（作业调度）：
  - 多道批处理操作系统中，从输入系统的一批作业中按照预定的调度策略挑选若干作业进入内存，为其分配资源并创建对应的作业用户进程
  - 作业是任务实体，进程是完成任务的执行实体。**作业的概念多用于批处理操作系统，而进程用于各种多道程序设计系统**
- 中级调度
  - 根据内存资源情况决定内存中所能容纳的进程数目，并完成外存和内存中的进程对换工作（挂起）。起到短期均衡系统负载的作用
- 低级调度（进程调度/线程调度）
  - **根据某种原则决定就绪队列中哪个进程/线程获得处理器，并将处理器出让给它使用**

### 进程和线程区别

1. 进程是计算机中的程序关于某数据集合上的一次运行活动，进程是系统资源分配的最小单位，线程是系统任务调度和执行的最小单位
2. 进程是应用程序的执行实体，一个进程可以有多个线程。一个进程至少包括一个线程。
3. 进程有自己独立的空间，每创建一个进程，系统建立数据表维护代码段，堆栈段，数据段，这种操作非常昂贵。而线程是共享进程资源的，使用相同的地址空间，创建线程仅仅需要`堆栈指针`和`程序计数器`就可以了。因此CPU创建或切换一个线程花费远比进程小。
4. 进程的创建要调用fork或者vfork，线程的的调用pthread_create，进程结束他拥有的所有的线程也结束，线程的结束不会影响同个进程中的其他线程。
5. 线程之间的通信更方便，同一进程下的线程共享变量，而进程之间的通信需要以通信的方式(IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。

#### **进程的调度算法**

1. 先来先服务
2. 时间片轮转
3. 短作业优先
4. 优先级调度
5. 多级反馈队列调度算法MLFQ 

> 在 MLFQ 中维护了多个队列，每个队列具有不同的优先级。每个进程只能存在于其中的一个队列。而 MLFQ 在调度时**总是执行优先级更高的进程。在相同优先级的队列中采用轮转的方式进行调度**。
>
> 目的：**I/O 密集型频繁而短，CPU 密集型稀疏而长**
>
> 总结下来可以归纳为如下的五点规则：
>
> 1. A 优先级 > B 优先级，运行 A。
> 2. A 优先级 = B 优先级，轮转运行 A、B。
> 3. 进程默认情况下在高优先级。
> 4. 当进程耗尽其对应时间片，降低其优先级。
> 5. 每经过一段时间 S，将所有进程重新放入高优先级队列。
> 6. 高优先级时间片短，低优先级时间片长
>
> **一方面优化了周转时间，一方面带来了响应时间的降低，它会根据进程的运行情况动态地对调度顺序进行调整**。

####  **进程的通信方式**

1. 匿名管道：半双工的通信方式，存在于内存中，只能在亲缘关系的父子和兄弟进程间通信。

2. 有名管道：有名管道磁盘文件的方式存在，可以实现本机任意进程间的通信。

3. 消息队列：消息队列是存放消息的链表，存放在内核中，消息队列由标识符标记。消息队列可以实现消息的随机查询，也可以按照消息的类型读取，比FIFO更有优势。克服了信号量传递消息少，管道只能承载无格式字节流以及缓冲区大小受限。

4. 信号：通知某种事件已经发生。

5. 信号量：信号量是一个计算器，用于多进程对共享数据的访问，解决与同步相关问题并避免竞争条件。

6. 共享内存：使得多进程可以访问同一块内存空间，不同进程可以及时看到共享内存数据的更新。是最快的IPC方式

   每个进程有自己的进程控制块和地址空间，并且有一个与之对应的页表，负责将进程的虚拟地址与物理地址得映射，通过内存管理单元MMU进行管理。两个不同的虚拟地址映射到物理空间的同一块区域即为共享内存。

   ![img](https://gitee.com/a2547555298/imags/raw/master/共享内存)

7. 套接字：主要用在客户端和服务器**不同机器**之间的双向通信。

**线程的通信方式**

1. 互斥量（Mutex）：采用互斥对象机制，只有拥有互斥对象的线程才可以访问公共资源。比如JAVA中的synchronized关键字和各种Lock
2. 消息队列：比如安卓中的Handler，通过不断循环获取其他线程的发来的消息
3. 信号量(Semphares)：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。
4. 事件（Event）：Wait/Notify：通过通知操作的方式来保持多线程同步。

### **死锁**

指多个进程因竞争共享资源而造成的一种僵局，若无外力作用，这些进程都将永远不能再向前推进。

**死锁原因**

1. 资源竞争
2. 请求和释放资源顺序不当

#### **死锁条件**

1. 互斥
2. 循环等待
3. 不可抢夺
4. 请求和保持

#### **处理死锁方法**

##### **预防死锁**

预防死锁。系统按预定的策略为进程分配资源，这些分配策略能使死锁的四个必要条件之一不成立，从而使系统不产生死锁。

**破坏请求：** 一次性分配所有资源，之后不会再申请资源，如果资源不满足则不分配

**破坏保持：**资源运行完后马上释放

**破坏不可抢夺：**若进程获得了一部分资源，但需等待其他资源。则等待期间占用的资源先释放，需要时再重新申请

**破坏循环等待：**对资源进行排号，按照序号递增的顺序请求资源 。

##### **避免死锁**

死锁的避免。系统动态地测试资源分配情况，仅当能确保系统安全时才给进程分配资源。常用：银行家算法。

安全状态：存在一个安全序列使得每个进程所需资源都能得到满足。

##### 死锁的检测

操作系统定时判断系统是否出现了死锁，当有死锁发生时设法解除死锁。

#### 死锁的解除办法：

1. 重启系统
2. 剥夺资源（分为一次性剥夺全部资源，和逐步剥夺资源）
3. 撤销进程，可以直接撤消死锁状态代价最小进程，直至有足够的资源可用

### 内存

#### 存储器

- 随机存储器（RAM）：内存中最重要的一种，表示既可以从中读取数据，也可以写入数据。当机器关闭时，内存中的信息会丢失
- 只读存储器（ROM）：ROM 一般只能用于数据的读取，不能写入数据，但是当机器停电时，这些数据不会丢失
- 高速缓存（Cache）：Cache 也是我们经常见到的，它分为一级缓存（L1 Cache）、二级缓存（L2 Cache）、三级缓存（L3 Cache）这些数据，它**位于内存和 CPU 之间**，是一个读写**速度**比内存更快的存储器。当 CPU 向内存写入数据时，这些数据也会被写入高速缓存中。当 CPU 需要读取数据时，会直接从高速缓存中直接读取，当然，如需要的数据在Cache中没有，CPU会再去读取内存中的数据.

#### 虚拟内存

虚拟内存的目的是为了**让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存**。

操作系统将内存抽象成地址空间。虚拟内存使得应用程序认为它拥有一个连续的地址空间，它通常是被分隔成多个物理内存碎片，还有一部分存储在外部磁盘存储器上，在需要时进行数据交换,可以让程序可以拥有超过系统物理内存大小的可用内存空间。

#### 进程的内存分配 & 内存访问

- **进程是在虚拟内存中的**，每个进程运行时都会得到4G的虚拟内存。
- 进程内存访问
  1. 每次要访问地址空间上的某一个地址，都需要**把虚拟地址翻译为实际物理内存地址**
  2. **所有进程共享这整一块物理内存，每个进程只把自己目前需要的虚拟地址空间映射到物理内存上**
  3. 进程需要知道**哪些地址空间上的数据在物理内存上**，哪些不在（可能这部分存储在磁盘上），还有在物理内存上的哪里，这就需要通过**页表**来记录
  4. 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上（页面号），第二部分记录物理内存页的地址（偏移量）
  5. **当进程访问某个虚拟地址的时候，就会先去看页表，如果发现对应的数据不在物理内存上，就会发生缺页异常**
  6. **缺页异常**的处理过程，**操作系统立即阻塞该进程，并将硬盘里对应的页换入内存**，然后使该进程就绪，如果内存已经满了，没有空地方了，那就找一个页覆盖，至于具体覆盖的哪个页，就需要看操作系统选择的**页面置换[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)**

**页面置换算法：**

OPT 页面置换算法（最佳页面置换算法）：所选择的被换出的页面将是最长时间内不再被访问， 通常可以保证获得**最低的缺页率**。

FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。

LRU （Least Currently Used）页面置换算法（最近最久未使用页面置换算法）：将最近最久未使用的页面换出。需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到 链表表头。这样就能保证链表表尾的页面是最近最久未访问的。

LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）：该置换算法选择在之前时期使用最少的页面作为淘汰页。

#### 内存管理机制

- 操作系统的内存管理主要负责**内存的分配与回收**（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是**将逻辑地址转换成相应的物理地址**等功能也是操作系统内存管理做的事情
- **连续分配管理方式**：连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如**块式管理**
  - **块式管理**：将内存分为几个**固定大小**的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被**浪费**了。这些在每个块中未被利用的空间，我们称之为碎片
- **非连续分配管理方式**：非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中
  - **页式管理** ：把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。**页式管理通过页表对应逻辑地址和物理地址**
  - **段式管理** ： 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多 。但是，最重要的是**段是有实际意义的，每个段定义了一组逻辑信息**，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 **段式管理通过段表对应逻辑地址和物理地址**
  - **段页式管理机制** ：段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是**把主存先分成若干段，每个段又分成若干页**，也就是说**段页式管理机制中段与段之间以及段的内部的都是离散的**。

##### 分页和分段共同点和区别

- 共同点
  1. 分页机制和分段机制都是为了提高内存利用率，较少内存碎片
  2. **页和段都是离散存储的**，所以两者都是离散分配内存的方式。但是，**每个页和段中的内存是连续的**
- 分段和分页的不同
  1. **目的不同**：**分页是由于系统管理的需要**而不是用户的需要，它是信息的物理单位；**分段的目的是为了能更好地满足用户的需要**，它是信息的逻辑单位，它含有一组其意义相对完整的信息；
  2. **大小**不同：**页的大小固定且由系统决定**；而**段的长度却不固定，由其所完成的功能决定**；
  3. 地址空间不同： 段向用户提供二维地址空间；页向用户提供的是一维地址空间；
  4. 信息共享：段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制；
  5. **内存碎片**：页式存储管理的优点是没有外碎片（因为页的大小固定），但会产生内碎片（一个页可能填充不满）；而段式管理的优点是没有内碎片（因为段大小可变，改变段大小来消除内碎片）。但段换入换出时，会产生外碎片（比如4k的段换5k的段，会产生1k的外碎片）。

##### 基本分页储存管理方式

- 在分页内存管理中，很重要的两点是：**1. 虚拟地址到物理地址的转换要快【快表】；2. 解决虚拟地址空间大，页表也会很大的问题【多级分页】**
- 因为程序数据存储在不同的页面中，而页面又离散的分布在内存中，因此需要一个**页表来记录逻辑地址和实际存储地址之间的映射关系**，**以实现从页号到物理块号的映射**
- 由于页表也是存储在内存中的，因此访问分页系统中内存数据需要两次的内存访问【一次是从内存中访问页表，从中找到指定的物理块号，加上页内偏移得到实际物理地址；第二次就是根据第一次得到的物理地址访问内存取出数据】
- **为了减少两次访问内存导致的效率影响，分页管理中引入了快表机制**，当要访问内存数据的时候，首先将页号在快表中查询，如果查找到说明要访问的页表项在快表中，那么直接从快表中读取相应的物理块号；如果没有找到，那么访问内存中的页表，从页表中得到物理地址，同时将页表中的该映射表项添加到快表中
- **在某些计算机中如果内存的逻辑地址很大，将会导致程序的页表项会很多，而页表在内存中是连续存放的，所以相应的就需要较大的连续内存空间**。为了解决这个问题，可以采用**两级页表或者多级页表的方法**
- 其中外层页表一次性调入内存且连续存放，内层页表离散存放。相应的访问内存页表的时候需要一次地址变换，访问逻辑地址对应的物理地址的时候也需要一次地址变换，而且一共需要访问内存3次才可以读取一次数据

##### 基本分段储存管理方式

- 分页是为了提高内存利用率，**而分段是为了满足程序员在编写代码的时候的一些逻辑需求**【比如数据共享，数据保护，动态链接等】
- **分段内存管理当中，地址是二维的，一维是段号，一维是段内地址**；其中每个段的长度是不一样的，而且每个段内部都是从0开始编址的 **（二维表示一个X（段号）是连续的，一个Y（段内地址）是连续的，但段号+段内地址不连续。分页是（页号+页内偏移）是连续的）**
- 由于分段管理中，**每个段内部是连续内存分配，但是段和段之间是离散分配的**，因此也存在一个逻辑地址到物理地址的映射关系，相应的就是**段表机制**。段表中的每一个表项记录了该段在内存中的起始地址和该段的长度。段表可以放在内存中也可以放在寄存器中。
- 访问内存的时候根据段号和段表项的长度计算当前访问段在段表中的位置，然后访问段表，得到该段的物理地址，根据该物理地址以及段内偏移量就可以得到需要访问的内存。由于也是两次内存访问，所以分段管理中同样引入了[联想](https://www.nowcoder.com/jump/super-jump/word?word=联想)寄存器。

段页式内存管理

- **页式存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享**
- 段页式管理就是将程序分为多个逻辑段，在每个段里面又进行分页，即将分段和分页组合起来使用。
- 为了实现地址变换，系统为每个进程建立一张段表，而每个分段有一张页表（在一个进程中，段表只有一个，而页表可能有多个）
- 在进行地址变换时，**首先通过段表查到页表起始地址，然后通过页表找到页帧号，最后形成物理地址**。如图所示，**进行一次访问实际需要至少三次访问主存**，这里同样可以使用快表以加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。
- 三次内存访问
  1. 访问内存中的段表查到页表的起始地址
  2. 访问内存中的页表找到页帧号，形成物理地址
  3. 得到物理地址后，再一次访问内存，存取指令或者数据

### 用户态和内核态

操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序。根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：

- 用户态：用户态运行的进程可以直接读取用户程序的数据
- 内核态：内核态运行的进程或程序几乎可以访问计算机的任何资源，不受限制

运行的程序基本都是运行在用户态。如果我们调用操作系统提供的内核态级别的子功能那就需要系统调用。

系统调用：与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都 必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。

系统调用是操作系统为应用程序提供能够访问到内核态的资源的接口。

**用户态切换到内核态的几种方式**

- 系统调用: 系统调用是用户态**主动要求**切换到内核态的一种方式， 用户应用程序通过操作系统调用内核为上层应用程序开放的接口来执行程序。


- 异常：当 cpu 在执行用户态的应用程序时，发生了某些不可知的异常。 于是当前用户态的应用进程切换到处理此异常的内核的程序中去。比如缺页异常


- 硬件设备的中断: 当硬件设备完成用户请求后，会向 cpu 发出相应的中断信号，这时 cpu 会暂停执行下一条即将要执行的指令，转而去执行与中断信号对应的应用程序， 如果先前执行的指令是用户态下程序的指令，那么这个转换过程也是用户态到内核态的转换。

**那么为什么要有用户态和内核态呢？**

主要是访问能力的限制的考量，计算机中有一些比较危险的操作，比如设置时钟、内存清理，这些都需要在内核态下完成，如果随意进行这些操作，系统不安全。



#### select、poll 、epoll 

select,poll 和 epoll 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I/O 操作。

select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。

> select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制，poll 中的描述符是 pollfd 类型的数组；
>
> select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。
>
> 当某个进程调用 epoll_create() 方法时，内核会创建一个 eventpoll 对象。
>
> 创建 epoll 对象后，可以用 epoll_ctl() 向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。
>
> 就绪列表：epoll 使用双向链表来实现就绪队列，是一种能够快速插入和删除的数据结构。索引结构：epoll 使用红黑树去监听并维护所有文件描述符。
>
> epoll 的描述符事件有两种触发模式：LT（水平触发）和 ET（边沿触发）。
>
> 当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait()会再次通知进程。
>
> 和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。
>
> 边沿触发仅触发一次，水平触发会一直触发。

select每次调用都要传入关注的文件描述符，由于操作系统不知道哪个文件描述符有事件发生，所以需要轮询所有的文件描述符。

epoll只是在调用epoll_ctl时就将文件描述符传到内核中，之后不需要再传入，内核在文件描述符有事件发生时，会将这个描述符放入到专门的数据结构中，因此你调用epoll_wait时，这个数据结构有数据就返回，没有数据就等待超时时间到。

这里有两点提高效率的地方，一是每次查是否将文件描述符从应用复制到内核中。第二是采用轮询还是还是从就绪的表中获取数据

>你同时和100个妹子聊天，只要盯着列表顶部有小红点的行了，不用划一边列表挨个回。前者epoll，后者select

### 锁

#### 操作系统中的锁机制

- **互斥锁**：mutex，用于保证在任何时刻，都只能有一个线程访问该对象。只有取得互斥锁的进程才能进入临界区，无论读写，当获取锁操作失败时，线程会进入睡眠，等待锁释放时被唤醒
- 读写锁：rwlock，分为读锁和写锁。读写锁要根据进程进入临界区的具体行为（读，写）来决定锁的占用情况。这样锁的状态就有三种了：读加锁、写加锁、无锁。
  1. 无锁。读/写进程都可以进入；
  2. 读锁。读进程可以进入。写进程不可以进入；
  3. 写锁。读/写进程都不可以进入
- 自旋锁：spinlock，自旋锁是指在进程试图取得锁失败的时候选择忙等待而不是阻塞自己。
  - 选择忙等待的优点在于如果该进程在其自身的CPU时间片内拿到锁（说明锁占用时间都比较短），则相比阻塞少了上下文切换
  - 注意这里还有一个隐藏条件：**多处理器**。因为单个处理器的情况下，由于当前自旋进程占用着CPU，持有锁的进程只有等待自旋进程耗尽CPU时间才有机会执行，这样CPU就空转了
- RCU：read-copy-update，在修改数据时，首先需要读取数据，然后生成一个副本，对副本进行修改，修改完成后，再将老数据update成新的数据。【有点像 copy-on-write】
  - 使用RCU时，读者几乎不需要同步开销，既不需要获得锁，也不使用原子指令，不会导致锁竞争，因此就**不用考虑死锁问题**了。
  - 对于写者的同步开销较大，它需要复制被修改的数据，还必须使用锁机制同步并行其它写者的修改操作。
  - 在有大量读操作，少量写操作的情况下效率非常高。【读多写少】

